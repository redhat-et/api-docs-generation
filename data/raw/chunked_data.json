{
    "errors": {
        "markdown": "[ sigstore](../sigstore.html)\n\n## API Documentation\n\n  * Error\n    * diagnostics\n    * print_and_exit\n  * NetworkError\n    * diagnostics\n  * TUFError\n    * TUFError\n    * message\n    * diagnostics\n  * MetadataError\n    * diagnostics\n  * RootError\n    * diagnostics\n\n[ built with pdoc ](https://pdoc.dev \"pdoc: Python API documentation\ngenerator\")\n\n#  [sigstore](./../sigstore.html).errors\n\nExceptions.\n\nView Source\n    \n\nclass Error(builtins.Exception): View Source\n    \n\nBase sigstore exception type. Defines helpers for diagnostics.\n\ndef diagnostics(self) -> str: View Source\n    \n\nReturns human-friendly error information.\n\ndef print_and_exit(self, raise_error: bool = False) -> None: View Source\n    \n\nPrints all relevant error information to stderr and exits.\n\n##### Inherited Members\n\nbuiltins.Exception\n\n    Exception\n\nbuiltins.BaseException\n\n    with_traceback\n    add_note\n    args\n\nclass NetworkError(Error): View Source\n    \n\nRaised when a connectivity-related issue occurs.\n\ndef diagnostics(self) -> str: View Source\n    \n\nReturns diagnostics for the error.\n\n##### Inherited Members\n\nbuiltins.Exception\n\n    Exception\n\nError\n\n    print_and_exit\n\nbuiltins.BaseException\n\n    with_traceback\n    add_note\n    args\n\nclass TUFError(Error): View Source\n    \n\nRaised when a TUF error occurs.\n\nTUFError(message: str) View Source\n    \n\nConstructs a `TUFError`.\n\nmessage\n\ndef diagnostics(self) -> str: View Source\n    \n\nReturns diagnostics specialized to the wrapped TUF error.\n\n##### Inherited Members\n\nError\n\n    print_and_exit\n\nbuiltins.BaseException\n\n    with_traceback\n    add_note\n    args\n\nclass MetadataError(Error): View Source\n    \n\nRaised when TUF metadata does not conform to the expected structure.\n\ndef diagnostics(self) -> str: View Source\n    \n\nReturns diagnostics for the error.\n\n##### Inherited Members\n\nbuiltins.Exception\n\n    Exception\n\nError\n\n    print_and_exit\n\nbuiltins.BaseException\n\n    with_traceback\n    add_note\n    args\n\nclass RootError(Error): View Source\n    \n\nRaised when TUF cannot establish its root of trust.\n\ndef diagnostics(self) -> str: View Source\n    \n\nReturns diagnostics for the error.\n\n##### Inherited Members\n\nbuiltins.Exception\n\n    Exception\n\nError\n\n    print_and_exit\n\nbuiltins.BaseException\n\n    with_traceback\n    add_note\n    args\n\n",
        "code": [
            {
                "errors.py": "# Copyright 2023 The Sigstore Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nExceptions.\n\n\nimport sys\nfrom typing import Any, Mapping\n\n\nclass Error(Exception):\n    Base sigstore exception type. Defines helpers for diagnostics.\n\n    def diagnostics(self) -> str:\n        Returns human-friendly error information.\n\n        return An issue occurred.\n\n    def print_and_exit(self, raise_error: bool = False) -> None:\n        Prints all relevant error information to stderr and exits.\n\n        remind_verbose = (\n            \"Raising original exception:\"\n            if raise_error\n            else \"For detailed error information, run sigstore with the `--verbose` flag.\"\n        )\n\n        print(f\"{self.diagnostics()}\\n{remind_verbose}\", file=sys.stderr)\n\n        if raise_error:\n            # don't want \"during handling another exception\"\n            self.__suppress_context__ = True\n            raise self\n\n        sys.exit(1)\n\n\nclass NetworkError(Error):\n    Raised when a connectivity-related issue occurs.\n\n    def diagnostics(self) -> str:\n        Returns diagnostics for the error.\n\n        cause_ctx = (\n            f\n        Additional context:\n\n        {self.__cause__}\n        \n            if self.__cause__\n            else \"\"\n        )\n\n        return (\n            \\\n        A network issue occurred.\n\n        Check your internet connection and try again.\n        \n            + cause_ctx\n        )\n\n\nclass TUFError(Error):\n    Raised when a TUF error occurs.\n\n    def __init__(self, message: str):\n        Constructs a `TUFError`.\n        self.message = message\n\n    from tuf.api import exceptions\n\n    _details: Mapping[Any, str] = {\n        exceptions.DownloadError: NetworkError().diagnostics()\n    }\n\n    def diagnostics(self) -> str:\n        Returns diagnostics specialized to the wrapped TUF error.\n        details = TUFError._details.get(\n            type(self.__context__),\n            \"Please report this issue at <https://github.com/sigstore/sigstore-python/issues/new>.\",\n        )\n\n        return f\\\n        {self.message}.\n\n        {details}\n        \n\n\nclass MetadataError(Error):\n    Raised when TUF metadata does not conform to the expected structure.\n\n    def diagnostics(self) -> str:\n        Returns diagnostics for the error.\n        return f{str(self)}.\n\n\nclass RootError(Error):\n    Raised when TUF cannot establish its root of trust.\n\n    def diagnostics(self) -> str:\n        Returns diagnostics for the error.\n        return \\\n        Unable to establish root of trust.\n\n        This error may occur when the resources embedded in this distribution of sigstore-python are out of date.\n"
            }
        ],
        "code_chunks": {
            "imports": [
                "import sys"
            ],
            "functions": [],
            "classes": [
                "class Error(Exception):\n    \n\n    def diagnostics(self) -> str:\n        \n\n        return An issue occurred.\n\n    def print_and_exit(self, raise_error: bool = False) -> None:\n        \n\n        remind_verbose = (\n            \"Raising original exception:\"\n            if raise_error\n            else \"For detailed error information, run sigstore with the `--verbose` flag.\"\n        )\n\n        print(f\"{self.diagnostics()}\\n{remind_verbose}\", file=sys.stderr)\n\n        if raise_error:\n            # don't want \"during handling another exception\"\n            self.__suppress_context__ = True\n            raise self\n\n        sys.exit(1)",
                "class NetworkError(Error):\n    \n\n    def diagnostics(self) -> str:\n        \n\n        cause_ctx = (\n            f\n        Additional context:\n\n        {self.__cause__}\n        \n            if self.__cause__\n            else \"\"\n        )\n\n        return (\n            \\\n        A network issue occurred.\n\n        Check your internet connection and try again.\n        \n            + cause_ctx\n        )",
                "class TUFError(Error):\n    \n\n    def __init__(self, message: str):\n        \n        self.message = message\n\n    from tuf.api import exceptions\n\n    _details: Mapping[Any, str] = {\n        exceptions.DownloadError: NetworkError().diagnostics()\n    }\n\n    def diagnostics(self) -> str:\n        \n        details = TUFError._details.get(\n            type(self.__context__),\n            \"Please report this issue at <https://github.com/sigstore/sigstore-python/issues/new>.\",\n        )\n\n        return f\\\n        {self.message}.\n\n        {details}\n        ",
                "class MetadataError(Error):\n    \n\n    def diagnostics(self) -> str:\n        \n        return f{str(self)}.",
                "class RootError(Error):\n    \n\n    def diagnostics(self) -> str:\n        \n        return \\\n        Unable to establish root of trust.\n\n        This error may occur when the resources embedded in this distribution of sigstore-python are out of date."
            ],
            "documentation": [
                "\nExceptions.\n"
            ],
            "other": [
                "# Copyright 2023 The Sigstore Authors",
                "#",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");",
                "# you may not use this file except in compliance with the License.",
                "# You may obtain a copy of the License at",
                "#",
                "#      http://www.apache.org/licenses/LICENSE-2.0",
                "#",
                "# Unless required by applicable law or agreed to in writing, software",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "# See the License for the specific language governing permissions and",
                "# limitations under the License.",
                "from typing import Any, Mapping"
            ],
            "functions_code": [],
            "functions_docstrings": [],
            "classes_code": [
                "class Error(Exception):\n    \n\n    def diagnostics(self) -> str:\n        \n\n        return An issue occurred.\n\n    def print_and_exit(self, raise_error: bool = False) -> None:\n        \n\n        remind_verbose = (\n            \"Raising original exception:\"\n            if raise_error\n            else \"For detailed error information, run sigstore with the `--verbose` flag.\"\n        )\n\n        print(f\"{self.diagnostics()}\\n{remind_verbose}\", file=sys.stderr)\n\n        if raise_error:\n            # don't want \"during handling another exception\"\n            self.__suppress_context__ = True\n            raise self\n\n        sys.exit(1)",
                "class NetworkError(Error):\n    \n\n    def diagnostics(self) -> str:\n        \n\n        cause_ctx = (\n            f\n        Additional context:\n\n        {self.__cause__}\n        \n            if self.__cause__\n            else \"\"\n        )\n\n        return (\n            \\\n        A network issue occurred.\n\n        Check your internet connection and try again.\n        \n            + cause_ctx\n        )",
                "class TUFError(Error):\n    \n\n    def __init__(self, message: str):\n        \n        self.message = message\n\n    from tuf.api import exceptions\n\n    _details: Mapping[Any, str] = {\n        exceptions.DownloadError: NetworkError().diagnostics()\n    }\n\n    def diagnostics(self) -> str:\n        \n        details = TUFError._details.get(\n            type(self.__context__),\n            \"Please report this issue at <https://github.com/sigstore/sigstore-python/issues/new>.\",\n        )\n\n        return f\\\n        {self.message}.\n\n        {details}\n        ",
                "class MetadataError(Error):\n    \n\n    def diagnostics(self) -> str:\n        \n        return f{str(self)}.",
                "class RootError(Error):\n    \n\n    def diagnostics(self) -> str:\n        \n        return \\\n        Unable to establish root of trust.\n\n        This error may occur when the resources embedded in this distribution of sigstore-python are out of date."
            ],
            "classes_docstrings": [
                "Base sigstore exception type. Defines helpers for diagnostics.",
                "Returns human-friendly error information.",
                "Prints all relevant error information to stderr and exits.",
                "Raised when a connectivity-related issue occurs.",
                "Returns diagnostics for the error.",
                "Raised when a TUF error occurs.",
                "Constructs a `TUFError`.",
                "Returns diagnostics specialized to the wrapped TUF error.",
                "Raised when TUF metadata does not conform to the expected structure.",
                "Returns diagnostics for the error.",
                "Raised when TUF cannot establish its root of trust.",
                "Returns diagnostics for the error."
            ]
        }
    },
    "oidc": {
        "markdown": "[ sigstore](../sigstore.html)\n\n## API Documentation\n\n  * DEFAULT_OAUTH_ISSUER_URL\n  * STAGING_OAUTH_ISSUER_URL\n  * DEFAULT_AUDIENCE\n  * ExpiredIdentity\n  * IdentityToken\n    * IdentityToken\n    * in_validity_period\n    * identity\n    * issuer\n    * expected_certificate_subject\n  * IssuerError\n  * Issuer\n    * Issuer\n    * production\n    * staging\n    * identity_token\n  * IdentityError\n    * raise_from_id\n    * diagnostics\n  * detect_credential\n\n[ built with pdoc ](https://pdoc.dev \"pdoc: Python API documentation\ngenerator\")\n\n#  [sigstore](./../sigstore.html).oidc\n\nAPI for retrieving OIDC tokens.\n\nView Source\n    \n\nDEFAULT_OAUTH_ISSUER_URL = 'https://oauth2.sigstore.dev/auth'\n\nSTAGING_OAUTH_ISSUER_URL = 'https://oauth2.sigstage.dev/auth'\n\nDEFAULT_AUDIENCE = 'sigstore'\n\nclass ExpiredIdentity(builtins.Exception): View Source\n    \n\nAn error raised when an identity token is expired.\n\n##### Inherited Members\n\nbuiltins.Exception\n\n    Exception\n\nbuiltins.BaseException\n\n    with_traceback\n    add_note\n    args\n\nclass IdentityToken: View Source\n    \n\nAn OIDC \"identity\", corresponding to an underlying OIDC token with a sensible\nsubject, issuer, and audience for Sigstore purposes.\n\nIdentityToken(raw_token: str) View Source\n    \n\nCreate a new `IdentityToken` from the given OIDC token.\n\ndef in_validity_period(self) -> bool: View Source\n    \n\nReturns whether or not this `Identity` is currently within its self-stated\nvalidity period.\n\nNOTE: As noted in `Identity.__init__`, this is not a verifying wrapper; the\ncheck here only asserts whether the _unverified_ identity's claims are within\ntheir validity period.\n\nidentity: str\n\nReturns this `IdentityToken`'s underlying \"subject\".\n\nNote that this is **not** always the `sub` claim in the corresponding identity\ntoken: depending onm the token's issuer, it may be a _different_ claim, such\nas `email`. This corresponds to the Sigstore ecosystem's behavior, e.g. in\neach issued certificate's SAN.\n\nissuer: str\n\nReturns a URL identifying this `IdentityToken`'s issuer.\n\nexpected_certificate_subject: str\n\nReturns a URL identifying the **expected** subject for any Sigstore\ncertificate issued against this identity token.\n\nThe behavior of this field is slightly subtle: for non-federated identity\nproviders (like a token issued directly by Google's IdP) it should be exactly\nequivalent to `IdentityToken.issuer`. For federated issuers (like Sigstore's\nown federated IdP) it should be equivalent to the underlying federated\nissuer's URL, which is kept in an implementation-defined claim.\n\nThis attribute exists so that clients who wish to inspect the expected subject\nof their certificates can do so without relying on implementation-specific\nbehavior.\n\nclass IssuerError(builtins.Exception): View Source\n    \n\nRaised on any communication or format error with an OIDC issuer.\n\n##### Inherited Members\n\nbuiltins.Exception\n\n    Exception\n\nbuiltins.BaseException\n\n    with_traceback\n    add_note\n    args\n\nclass Issuer: View Source\n    \n\nRepresents an OIDC issuer (IdP).\n\nIssuer(base_url: str) View Source\n    \n\nCreate a new `Issuer` from the given base URL.\n\nThis URL is used to locate an OpenID Connect configuration file, which is then\nused to bootstrap the issuer's state (such as authorization and token\nendpoints).\n\n@classmethod\n\ndef production(cls) -> Issuer: View Source\n    \n\nReturns an `Issuer` configured against Sigstore's production-level services.\n\n@classmethod\n\ndef staging(cls) -> Issuer: View Source\n    \n\nReturns an `Issuer` configured against Sigstore's staging-level services.\n\ndef identity_token( self, client_id: str = 'sigstore', client_secret: str =\n'', force_oob: bool = False) -> IdentityToken: View Source\n    \n\nRetrieves and returns an `IdentityToken` from the current `Issuer`, via OAuth.\n\nThis function blocks on user interaction.\n\nThe `force_oob` flag controls the kind of flow performed. When `False` (the\ndefault), this function attempts to open the user's web browser before falling\nback to an out-of-band flow. When `True`, the out-of-band flow is always used.\n\nclass IdentityError([sigstore.errors.Error](errors.html#Error)): View Source\n    \n\nWraps `id`'s IdentityError.\n\n@classmethod\n\ndef raise_from_id(cls, exc: id.IdentityError) -> NoReturn: View Source\n    \n\nRaises a wrapped IdentityError from the provided `id.IdentityError`.\n\ndef diagnostics(self) -> str: View Source\n    \n\nReturns diagnostics for the error.\n\n##### Inherited Members\n\nbuiltins.Exception\n\n    Exception\n\n[sigstore.errors.Error](errors.html#Error)\n\n    [print_and_exit](errors.html#Error.print_and_exit)\n\nbuiltins.BaseException\n\n    with_traceback\n    add_note\n    args\n\ndef detect_credential() -> Optional[str]: View Source\n    \n\nCalls `id.detect_credential`, but wraps exceptions with our own exception\ntype.\n\n",
        "code": [
            {
                "oidc.py": "# Copyright 2022 The Sigstore Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nAPI for retrieving OIDC tokens.\n\n\nfrom __future__ import annotations\n\nimport logging\nimport sys\nimport time\nimport urllib.parse\nimport webbrowser\nfrom datetime import datetime, timezone\nfrom typing import NoReturn, Optional, cast\n\nimport id\nimport jwt\nimport requests\nfrom pydantic import BaseModel, StrictStr\n\nfrom sigstore.errors import Error, NetworkError\n\nDEFAULT_OAUTH_ISSUER_URL = \"https://oauth2.sigstore.dev/auth\"\nSTAGING_OAUTH_ISSUER_URL = \"https://oauth2.sigstage.dev/auth\"\n\n# See: https://github.com/sigstore/fulcio/blob/b2186c0/pkg/config/config.go#L182-L201\n_KNOWN_OIDC_ISSUERS = {\n    \"https://accounts.google.com\": \"email\",\n    \"https://oauth2.sigstore.dev/auth\": \"email\",\n    \"https://oauth2.sigstage.dev/auth\": \"email\",\n    \"https://token.actions.githubusercontent.com\": \"sub\",\n}\n_DEFAULT_AUDIENCE = \"sigstore\"\n\n\nclass _OpenIDConfiguration(BaseModel):\n    \n    Represents a (subset) of the fields provided by an OpenID Connect provider's\n    `.well-known/openid-configuration` response, as defined by OpenID Connect Discovery.\n\n    See: <https://openid.net/specs/openid-connect-discovery-1_0.html>\n    \n\n    authorization_endpoint: StrictStr\n    token_endpoint: StrictStr\n\n\n# See: https://github.com/sigstore/fulcio/blob/b2186c0/pkg/config/config.go#L182-L201\n_KNOWN_OIDC_ISSUERS = {\n    \"https://accounts.google.com\": \"email\",\n    \"https://oauth2.sigstore.dev/auth\": \"email\",\n    \"https://oauth2.sigstage.dev/auth\": \"email\",\n    \"https://token.actions.githubusercontent.com\": \"sub\",\n}\nDEFAULT_AUDIENCE = \"sigstore\"\n\n\nclass ExpiredIdentity(Exception):\n    An error raised when an identity token is expired.\n\n\nclass IdentityToken:\n    \n    An OIDC \"identity\", corresponding to an underlying OIDC token with\n    a sensible subject, issuer, and audience for Sigstore purposes.\n    \n\n    def __init__(self, raw_token: str) -> None:\n        \n        Create a new `IdentityToken` from the given OIDC token.\n        \n\n        self._raw_token = raw_token\n\n        # NOTE: The lack of verification here is intentional, and is part of\n        # Sigstore's verification model: clients like sigstore-python are\n        # responsible only for forwarding the OIDC identity to Fulcio for\n        # certificate binding and issuance.\n        try:\n            self._unverified_claims = jwt.decode(\n                raw_token,\n                options={\n                    \"verify_signature\": False,\n                    \"verify_aud\": True,\n                    \"verify_iat\": True,\n                    \"verify_exp\": True,\n                    # These claims are required by OpenID Connect, so\n                    # we can strongly enforce their presence.\n                    # See: https://openid.net/specs/openid-connect-basic-1_0.html#IDToken\n                    \"require\": [\"aud\", \"sub\", \"iat\", \"exp\", \"iss\"],\n                },\n                audience=DEFAULT_AUDIENCE,\n                # NOTE: This leeway shouldn't be strictly necessary, but is\n                # included to preempt any (small) skew between the host\n                # and the originating IdP.\n                leeway=5,\n            )\n        except Exception as exc:\n            raise IdentityError(\n                \"Identity token is malformed or missing claims\"\n            ) from exc\n\n        self._iss: str = self._unverified_claims[\"iss\"]\n        self._nbf: int | None = self._unverified_claims.get(\"nbf\")\n        self._exp: int = self._unverified_claims[\"exp\"]\n\n        # Fail early if this token isn't within its validity period.\n        if not self.in_validity_period():\n            raise IdentityError(\"Identity token is not within its validity period\")\n\n        # When verifying the private key possession proof, Fulcio uses\n        # different claims depending on the token's issuer.\n        # We currently special-case a handful of these, and fall back\n        # on signing the \"sub\" claim otherwise.\n        identity_claim = _KNOWN_OIDC_ISSUERS.get(self.issuer)\n        if identity_claim is not None:\n            if identity_claim not in self._unverified_claims:\n                raise IdentityError(\n                    f\"Identity token is missing the required {identity_claim!r} claim\"\n                )\n\n            self._identity = str(self._unverified_claims.get(identity_claim))\n        else:\n            try:\n                self._identity = str(self._unverified_claims[\"sub\"])\n            except KeyError:\n                raise IdentityError(\n                    \"Identity token is missing the required 'sub' claim\"\n                )\n\n        # This identity token might have been retrieved directly from\n        # an identity provider, or it might be a \"federated\" identity token\n        # retrieved from a federated IdP (e.g., Sigstore's own Dex instance).\n        # In the latter case, the claims will also include a `federated_claims`\n        # set, which in turn should include a `connector_id` that reflects\n        # the \"real\" token issuer. We retrieve this, despite technically\n        # being an implementation detail, because it has value to client\n        # users: a client might want to make sure that its user is identifying\n        # with a *particular* IdP, which means that they need to pierce the\n        # federation layer to check which IdP is actually being used.\n        self._federated_issuer: str | None = None\n        federated_claims = self._unverified_claims.get(\"federated_claims\")\n        if federated_claims is not None:\n            if not isinstance(federated_claims, dict):\n                raise IdentityError(\n                    \"unexpected claim type: federated_claims is not a dict\"\n                )\n\n            federated_issuer = federated_claims.get(\"connector_id\")\n            if federated_issuer is not None:\n                if not isinstance(federated_issuer, str):\n                    raise IdentityError(\n                        \"unexpected claim type: federated_claims.connector_id is not a string\"\n                    )\n\n                self._federated_issuer = federated_issuer\n\n    def in_validity_period(self) -> bool:\n        \n        Returns whether or not this `Identity` is currently within its self-stated validity period.\n\n        NOTE: As noted in `Identity.__init__`, this is not a verifying wrapper;\n        the check here only asserts whether the *unverified* identity's claims\n        are within their validity period.\n        \n\n        now = datetime.now(timezone.utc).timestamp()\n\n        if self._nbf is not None:\n            return self._nbf <= now < self._exp\n        else:\n            return now < self._exp\n\n    @property\n    def identity(self) -> str:\n        \n        Returns this `IdentityToken`'s underlying \"subject\".\n\n        Note that this is **not** always the `sub` claim in the corresponding\n        identity token: depending onm the token's issuer, it may be a *different*\n        claim, such as `email`. This corresponds to the Sigstore ecosystem's\n        behavior, e.g. in each issued certificate's SAN.\n        \n        return self._identity\n\n    @property\n    def issuer(self) -> str:\n        \n        Returns a URL identifying this `IdentityToken`'s issuer.\n        \n        return self._iss\n\n    @property\n    def expected_certificate_subject(self) -> str:\n        \n        Returns a URL identifying the **expected** subject for any Sigstore\n        certificate issued against this identity token.\n\n        The behavior of this field is slightly subtle: for non-federated\n        identity providers (like a token issued directly by Google's IdP) it\n        should be exactly equivalent to `IdentityToken.issuer`. For federated\n        issuers (like Sigstore's own federated IdP) it should be equivalent to\n        the underlying federated issuer's URL, which is kept in an\n        implementation-defined claim.\n\n        This attribute exists so that clients who wish to inspect the expected\n        subject of their certificates can do so without relying on\n        implementation-specific behavior.\n        \n        if self._federated_issuer is not None:\n            return self._federated_issuer\n\n        return self.issuer\n\n    def __str__(self) -> str:\n        \n        Returns the underlying OIDC token for this identity.\n\n        That this token is secret in nature and **MUST NOT** be disclosed.\n        \n        return self._raw_token\n\n\nclass IssuerError(Exception):\n    \n    Raised on any communication or format error with an OIDC issuer.\n    \n\n    pass\n\n\nclass Issuer:\n    \n    Represents an OIDC issuer (IdP).\n    \n\n    def __init__(self, base_url: str) -> None:\n        \n        Create a new `Issuer` from the given base URL.\n\n        This URL is used to locate an OpenID Connect configuration file,\n        which is then used to bootstrap the issuer's state (such\n        as authorization and token endpoints).\n        \n        oidc_config_url = urllib.parse.urljoin(\n            f\"{base_url}/\", \".well-known/openid-configuration\"\n        )\n\n        try:\n            resp: requests.Response = requests.get(oidc_config_url, timeout=30)\n        except (requests.ConnectionError, requests.Timeout) as exc:\n            raise NetworkError from exc\n\n        try:\n            resp.raise_for_status()\n        except requests.HTTPError as http_error:\n            raise IssuerError from http_error\n\n        try:\n            # We don't generally expect this to fail (since the provider should\n            # return a non-success HTTP code which we catch above), but we\n            # check just in case we have a misbehaving OIDC issuer.\n            self.oidc_config = _OpenIDConfiguration.model_validate(resp.json())\n        except ValueError as exc:\n            raise IssuerError(f\"OIDC issuer returned invalid configuration: {exc}\")\n\n    @classmethod\n    def production(cls) -> Issuer:\n        \n        Returns an `Issuer` configured against Sigstore's production-level services.\n        \n        return cls(DEFAULT_OAUTH_ISSUER_URL)\n\n    @classmethod\n    def staging(cls) -> Issuer:\n        \n        Returns an `Issuer` configured against Sigstore's staging-level services.\n        \n        return cls(STAGING_OAUTH_ISSUER_URL)\n\n    def identity_token(  # nosec: B107\n        self,\n        client_id: str = \"sigstore\",\n        client_secret: str = \"\",\n        force_oob: bool = False,\n    ) -> IdentityToken:\n        \n        Retrieves and returns an `IdentityToken` from the current `Issuer`, via OAuth.\n\n        This function blocks on user interaction.\n\n        The `force_oob` flag controls the kind of flow performed. When `False` (the default),\n        this function attempts to open the user's web browser before falling back to\n        an out-of-band flow. When `True`, the out-of-band flow is always used.\n        \n\n        # This function and the components that it relies on are based off of:\n        # https://github.com/psteniusubi/python-sample\n\n        from sigstore._internal.oidc.oauth import _OAuthFlow\n\n        code: str\n        with _OAuthFlow(client_id, client_secret, self) as server:\n            # Launch web browser\n            if not force_oob and webbrowser.open(server.base_uri):\n                print(\"Waiting for browser interaction...\", file=sys.stderr)\n            else:\n                server.enable_oob()\n                print(\n                    f\"Go to the following link in a browser:\\n\\n\\t{server.auth_endpoint}\",\n                    file=sys.stderr,\n                )\n\n            if not server.is_oob():\n                # Wait until the redirect server populates the response\n                while server.auth_response is None:\n                    time.sleep(0.1)\n\n                auth_error = server.auth_response.get(\"error\")\n                if auth_error is not None:\n                    raise IdentityError(\n                        f\"Error response from auth endpoint: {auth_error[0]}\"\n                    )\n                code = server.auth_response[\"code\"][0]\n            else:\n                # In the out-of-band case, we wait until the user provides the code\n                code = input(\"Enter verification code: \")\n\n        # Provide code to token endpoint\n        data = {\n            \"grant_type\": \"authorization_code\",\n            \"redirect_uri\": server.redirect_uri,\n            \"code\": code,\n            \"code_verifier\": server.oauth_session.code_verifier,\n        }\n        auth = (\n            client_id,\n            client_secret,\n        )\n        logging.debug(f\"PAYLOAD: data={data}\")\n        try:\n            resp: requests.Response = requests.post(\n                self.oidc_config.token_endpoint,\n                data=data,\n                auth=auth,\n                timeout=30,\n            )\n        except (requests.ConnectionError, requests.Timeout) as exc:\n            raise NetworkError from exc\n\n        try:\n            resp.raise_for_status()\n        except requests.HTTPError as http_error:\n            raise IdentityError(\n                f\"Token request failed with {resp.status_code}\"\n            ) from http_error\n\n        token_json = resp.json()\n        token_error = token_json.get(\"error\")\n        if token_error is not None:\n            raise IdentityError(f\"Error response from token endpoint: {token_error}\")\n\n        return IdentityToken(token_json[\"access_token\"])\n\n\nclass IdentityError(Error):\n    \n    Wraps `id`'s IdentityError.\n    \n\n    @classmethod\n    def raise_from_id(cls, exc: id.IdentityError) -> NoReturn:\n        Raises a wrapped IdentityError from the provided `id.IdentityError`.\n        raise cls(str(exc)) from exc\n\n    def diagnostics(self) -> str:\n        Returns diagnostics for the error.\n        if isinstance(self.__cause__, id.GitHubOidcPermissionCredentialError):\n            return f\n                Insufficient permissions for GitHub Actions workflow.\n\n                The most common reason for this is incorrect\n                configuration of the top-level `permissions` setting of the\n                workflow YAML file. It should be configured like so:\n\n                    permissions:\n                      id-token: write\n\n                Relevant documentation here:\n\n                    https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect#adding-permissions-settings\n\n                Another possible reason is that the workflow run has been\n                triggered by a PR from a forked repository. PRs from forked\n                repositories typically cannot be granted write access.\n\n                Relevant documentation here:\n\n                    https://docs.github.com/en/actions/security-guides/automatic-token-authentication#modifying-the-permissions-for-the-github_token\n\n                Additional context:\n\n                {self.__cause__}\n                \n        else:\n            return f\n                An issue occurred with ambient credential detection.\n\n                Additional context:\n\n                {self}\n            \n\n\ndef detect_credential() -> Optional[str]:\n    Calls `id.detect_credential`, but wraps exceptions with our own exception type.\n    try:\n        return cast(Optional[str], id.detect_credential(_DEFAULT_AUDIENCE))\n    except id.IdentityError as exc:\n        IdentityError.raise_from_id(exc)\n"
            }
        ],
        "code_chunks": {
            "imports": [
                "import logging",
                "import sys",
                "import time",
                "import urllib.parse",
                "import webbrowser",
                "import id",
                "import jwt",
                "import requests"
            ],
            "functions": [
                "def detect_credential() -> Optional[str]:\n    \n    try:\n        return cast(Optional[str], id.detect_credential(_DEFAULT_AUDIENCE))\n    except id.IdentityError as exc:\n        IdentityError.raise_from_id(exc)"
            ],
            "classes": [
                "class _OpenIDConfiguration(BaseModel):\n    \n\n    authorization_endpoint: StrictStr\n    token_endpoint: StrictStr",
                "class ExpiredIdentity(Exception):\n    ",
                "class IdentityToken:\n    \n\n    def __init__(self, raw_token: str) -> None:\n        \n\n        self._raw_token = raw_token\n\n        # NOTE: The lack of verification here is intentional, and is part of\n        # Sigstore's verification model: clients like sigstore-python are\n        # responsible only for forwarding the OIDC identity to Fulcio for\n        # certificate binding and issuance.\n        try:\n            self._unverified_claims = jwt.decode(\n                raw_token,\n                options={\n                    \"verify_signature\": False,\n                    \"verify_aud\": True,\n                    \"verify_iat\": True,\n                    \"verify_exp\": True,\n                    # These claims are required by OpenID Connect, so\n                    # we can strongly enforce their presence.\n                    # See: https://openid.net/specs/openid-connect-basic-1_0.html#IDToken\n                    \"require\": [\"aud\", \"sub\", \"iat\", \"exp\", \"iss\"],\n                },\n                audience=DEFAULT_AUDIENCE,\n                # NOTE: This leeway shouldn't be strictly necessary, but is\n                # included to preempt any (small) skew between the host\n                # and the originating IdP.\n                leeway=5,\n            )\n        except Exception as exc:\n            raise IdentityError(\n                \"Identity token is malformed or missing claims\"\n            ) from exc\n\n        self._iss: str = self._unverified_claims[\"iss\"]\n        self._nbf: int | None = self._unverified_claims.get(\"nbf\")\n        self._exp: int = self._unverified_claims[\"exp\"]\n\n        # Fail early if this token isn't within its validity period.\n        if not self.in_validity_period():\n            raise IdentityError(\"Identity token is not within its validity period\")\n\n        # When verifying the private key possession proof, Fulcio uses\n        # different claims depending on the token's issuer.\n        # We currently special-case a handful of these, and fall back\n        # on signing the \"sub\" claim otherwise.\n        identity_claim = _KNOWN_OIDC_ISSUERS.get(self.issuer)\n        if identity_claim is not None:\n            if identity_claim not in self._unverified_claims:\n                raise IdentityError(\n                    f\"Identity token is missing the required {identity_claim!r} claim\"\n                )\n\n            self._identity = str(self._unverified_claims.get(identity_claim))\n        else:\n            try:\n                self._identity = str(self._unverified_claims[\"sub\"])\n            except KeyError:\n                raise IdentityError(\n                    \"Identity token is missing the required 'sub' claim\"\n                )\n\n        # This identity token might have been retrieved directly from\n        # an identity provider, or it might be a \"federated\" identity token\n        # retrieved from a federated IdP (e.g., Sigstore's own Dex instance).\n        # In the latter case, the claims will also include a `federated_claims`\n        # set, which in turn should include a `connector_id` that reflects\n        # the \"real\" token issuer. We retrieve this, despite technically\n        # being an implementation detail, because it has value to client\n        # users: a client might want to make sure that its user is identifying\n        # with a *particular* IdP, which means that they need to pierce the\n        # federation layer to check which IdP is actually being used.\n        self._federated_issuer: str | None = None\n        federated_claims = self._unverified_claims.get(\"federated_claims\")\n        if federated_claims is not None:\n            if not isinstance(federated_claims, dict):\n                raise IdentityError(\n                    \"unexpected claim type: federated_claims is not a dict\"\n                )\n\n            federated_issuer = federated_claims.get(\"connector_id\")\n            if federated_issuer is not None:\n                if not isinstance(federated_issuer, str):\n                    raise IdentityError(\n                        \"unexpected claim type: federated_claims.connector_id is not a string\"\n                    )\n\n                self._federated_issuer = federated_issuer\n\n    def in_validity_period(self) -> bool:\n        \n\n        now = datetime.now(timezone.utc).timestamp()\n\n        if self._nbf is not None:\n            return self._nbf <= now < self._exp\n        else:\n            return now < self._exp\n\n    @property\n    def identity(self) -> str:\n        \n        return self._identity\n\n    @property\n    def issuer(self) -> str:\n        \n        return self._iss\n\n    @property\n    def expected_certificate_subject(self) -> str:\n        \n        if self._federated_issuer is not None:\n            return self._federated_issuer\n\n        return self.issuer\n\n    def __str__(self) -> str:\n        \n        return self._raw_token",
                "class IssuerError(Exception):\n    \n\n    pass",
                "class Issuer:\n    \n\n    def __init__(self, base_url: str) -> None:\n        \n        oidc_config_url = urllib.parse.urljoin(\n            f\"{base_url}/\", \".well-known/openid-configuration\"\n        )\n\n        try:\n            resp: requests.Response = requests.get(oidc_config_url, timeout=30)\n        except (requests.ConnectionError, requests.Timeout) as exc:\n            raise NetworkError from exc\n\n        try:\n            resp.raise_for_status()\n        except requests.HTTPError as http_error:\n            raise IssuerError from http_error\n\n        try:\n            # We don't generally expect this to fail (since the provider should\n            # return a non-success HTTP code which we catch above), but we\n            # check just in case we have a misbehaving OIDC issuer.\n            self.oidc_config = _OpenIDConfiguration.model_validate(resp.json())\n        except ValueError as exc:\n            raise IssuerError(f\"OIDC issuer returned invalid configuration: {exc}\")\n\n    @classmethod\n    def production(cls) -> Issuer:\n        \n        return cls(DEFAULT_OAUTH_ISSUER_URL)\n\n    @classmethod\n    def staging(cls) -> Issuer:\n        \n        return cls(STAGING_OAUTH_ISSUER_URL)\n\n    def identity_token(  # nosec: B107\n        self,\n        client_id: str = \"sigstore\",\n        client_secret: str = \"\",\n        force_oob: bool = False,\n    ) -> IdentityToken:\n        \n\n        # This function and the components that it relies on are based off of:\n        # https://github.com/psteniusubi/python-sample\n\n        from sigstore._internal.oidc.oauth import _OAuthFlow\n\n        code: str\n        with _OAuthFlow(client_id, client_secret, self) as server:\n            # Launch web browser\n            if not force_oob and webbrowser.open(server.base_uri):\n                print(\"Waiting for browser interaction...\", file=sys.stderr)\n            else:\n                server.enable_oob()\n                print(\n                    f\"Go to the following link in a browser:\\n\\n\\t{server.auth_endpoint}\",\n                    file=sys.stderr,\n                )\n\n            if not server.is_oob():\n                # Wait until the redirect server populates the response\n                while server.auth_response is None:\n                    time.sleep(0.1)\n\n                auth_error = server.auth_response.get(\"error\")\n                if auth_error is not None:\n                    raise IdentityError(\n                        f\"Error response from auth endpoint: {auth_error[0]}\"\n                    )\n                code = server.auth_response[\"code\"][0]\n            else:\n                # In the out-of-band case, we wait until the user provides the code\n                code = input(\"Enter verification code: \")\n\n        # Provide code to token endpoint\n        data = {\n            \"grant_type\": \"authorization_code\",\n            \"redirect_uri\": server.redirect_uri,\n            \"code\": code,\n            \"code_verifier\": server.oauth_session.code_verifier,\n        }\n        auth = (\n            client_id,\n            client_secret,\n        )\n        logging.debug(f\"PAYLOAD: data={data}\")\n        try:\n            resp: requests.Response = requests.post(\n                self.oidc_config.token_endpoint,\n                data=data,\n                auth=auth,\n                timeout=30,\n            )\n        except (requests.ConnectionError, requests.Timeout) as exc:\n            raise NetworkError from exc\n\n        try:\n            resp.raise_for_status()\n        except requests.HTTPError as http_error:\n            raise IdentityError(\n                f\"Token request failed with {resp.status_code}\"\n            ) from http_error\n\n        token_json = resp.json()\n        token_error = token_json.get(\"error\")\n        if token_error is not None:\n            raise IdentityError(f\"Error response from token endpoint: {token_error}\")\n\n        return IdentityToken(token_json[\"access_token\"])",
                "class IdentityError(Error):\n    \n\n    @classmethod\n    def raise_from_id(cls, exc: id.IdentityError) -> NoReturn:\n        \n        raise cls(str(exc)) from exc\n\n    def diagnostics(self) -> str:\n        \n        if isinstance(self.__cause__, id.GitHubOidcPermissionCredentialError):\n            return f\n                Insufficient permissions for GitHub Actions workflow.\n\n                The most common reason for this is incorrect\n                configuration of the top-level `permissions` setting of the\n                workflow YAML file. It should be configured like so:\n\n                    permissions:\n                      id-token: write\n\n                Relevant documentation here:\n\n                    https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect#adding-permissions-settings\n\n                Another possible reason is that the workflow run has been\n                triggered by a PR from a forked repository. PRs from forked\n                repositories typically cannot be granted write access.\n\n                Relevant documentation here:\n\n                    https://docs.github.com/en/actions/security-guides/automatic-token-authentication#modifying-the-permissions-for-the-github_token\n\n                Additional context:\n\n                {self.__cause__}\n                \n        else:\n            return f\n                An issue occurred with ambient credential detection.\n\n                Additional context:\n\n                {self}\n            "
            ],
            "documentation": [
                "\nAPI for retrieving OIDC tokens.\n"
            ],
            "other": [
                "# Copyright 2022 The Sigstore Authors",
                "#",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");",
                "# you may not use this file except in compliance with the License.",
                "# You may obtain a copy of the License at",
                "#",
                "#      http://www.apache.org/licenses/LICENSE-2.0",
                "#",
                "# Unless required by applicable law or agreed to in writing, software",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "# See the License for the specific language governing permissions and",
                "# limitations under the License.",
                "from __future__ import annotations",
                "from datetime import datetime, timezone",
                "from typing import NoReturn, Optional, cast",
                "from pydantic import BaseModel, StrictStr",
                "from sigstore.errors import Error, NetworkError",
                "DEFAULT_OAUTH_ISSUER_URL = \"https://oauth2.sigstore.dev/auth\"",
                "STAGING_OAUTH_ISSUER_URL = \"https://oauth2.sigstage.dev/auth\"",
                "# See: https://github.com/sigstore/fulcio/blob/b2186c0/pkg/config/config.go#L182-L201",
                "_KNOWN_OIDC_ISSUERS = {\n    \"https://accounts.google.com\": \"email\",\n    \"https://oauth2.sigstore.dev/auth\": \"email\",\n    \"https://oauth2.sigstage.dev/auth\": \"email\",\n    \"https://token.actions.githubusercontent.com\": \"sub\",\n}",
                "_DEFAULT_AUDIENCE = \"sigstore\"",
                "# See: https://github.com/sigstore/fulcio/blob/b2186c0/pkg/config/config.go#L182-L201",
                "_KNOWN_OIDC_ISSUERS = {\n    \"https://accounts.google.com\": \"email\",\n    \"https://oauth2.sigstore.dev/auth\": \"email\",\n    \"https://oauth2.sigstage.dev/auth\": \"email\",\n    \"https://token.actions.githubusercontent.com\": \"sub\",\n}",
                "DEFAULT_AUDIENCE = \"sigstore\""
            ],
            "functions_code": [
                "def detect_credential() -> Optional[str]:\n    \n    try:\n        return cast(Optional[str], id.detect_credential(_DEFAULT_AUDIENCE))\n    except id.IdentityError as exc:\n        IdentityError.raise_from_id(exc)"
            ],
            "functions_docstrings": [
                "Calls `id.detect_credential`, but wraps exceptions with our own exception type."
            ],
            "classes_code": [
                "class _OpenIDConfiguration(BaseModel):\n    \n\n    authorization_endpoint: StrictStr\n    token_endpoint: StrictStr",
                "class ExpiredIdentity(Exception):\n    ",
                "class IdentityToken:\n    \n\n    def __init__(self, raw_token: str) -> None:\n        \n\n        self._raw_token = raw_token\n\n        # NOTE: The lack of verification here is intentional, and is part of\n        # Sigstore's verification model: clients like sigstore-python are\n        # responsible only for forwarding the OIDC identity to Fulcio for\n        # certificate binding and issuance.\n        try:\n            self._unverified_claims = jwt.decode(\n                raw_token,\n                options={\n                    \"verify_signature\": False,\n                    \"verify_aud\": True,\n                    \"verify_iat\": True,\n                    \"verify_exp\": True,\n                    # These claims are required by OpenID Connect, so\n                    # we can strongly enforce their presence.\n                    # See: https://openid.net/specs/openid-connect-basic-1_0.html#IDToken\n                    \"require\": [\"aud\", \"sub\", \"iat\", \"exp\", \"iss\"],\n                },\n                audience=DEFAULT_AUDIENCE,\n                # NOTE: This leeway shouldn't be strictly necessary, but is\n                # included to preempt any (small) skew between the host\n                # and the originating IdP.\n                leeway=5,\n            )\n        except Exception as exc:\n            raise IdentityError(\n                \"Identity token is malformed or missing claims\"\n            ) from exc\n\n        self._iss: str = self._unverified_claims[\"iss\"]\n        self._nbf: int | None = self._unverified_claims.get(\"nbf\")\n        self._exp: int = self._unverified_claims[\"exp\"]\n\n        # Fail early if this token isn't within its validity period.\n        if not self.in_validity_period():\n            raise IdentityError(\"Identity token is not within its validity period\")\n\n        # When verifying the private key possession proof, Fulcio uses\n        # different claims depending on the token's issuer.\n        # We currently special-case a handful of these, and fall back\n        # on signing the \"sub\" claim otherwise.\n        identity_claim = _KNOWN_OIDC_ISSUERS.get(self.issuer)\n        if identity_claim is not None:\n            if identity_claim not in self._unverified_claims:\n                raise IdentityError(\n                    f\"Identity token is missing the required {identity_claim!r} claim\"\n                )\n\n            self._identity = str(self._unverified_claims.get(identity_claim))\n        else:\n            try:\n                self._identity = str(self._unverified_claims[\"sub\"])\n            except KeyError:\n                raise IdentityError(\n                    \"Identity token is missing the required 'sub' claim\"\n                )\n\n        # This identity token might have been retrieved directly from\n        # an identity provider, or it might be a \"federated\" identity token\n        # retrieved from a federated IdP (e.g., Sigstore's own Dex instance).\n        # In the latter case, the claims will also include a `federated_claims`\n        # set, which in turn should include a `connector_id` that reflects\n        # the \"real\" token issuer. We retrieve this, despite technically\n        # being an implementation detail, because it has value to client\n        # users: a client might want to make sure that its user is identifying\n        # with a *particular* IdP, which means that they need to pierce the\n        # federation layer to check which IdP is actually being used.\n        self._federated_issuer: str | None = None\n        federated_claims = self._unverified_claims.get(\"federated_claims\")\n        if federated_claims is not None:\n            if not isinstance(federated_claims, dict):\n                raise IdentityError(\n                    \"unexpected claim type: federated_claims is not a dict\"\n                )\n\n            federated_issuer = federated_claims.get(\"connector_id\")\n            if federated_issuer is not None:\n                if not isinstance(federated_issuer, str):\n                    raise IdentityError(\n                        \"unexpected claim type: federated_claims.connector_id is not a string\"\n                    )\n\n                self._federated_issuer = federated_issuer\n\n    def in_validity_period(self) -> bool:\n        \n\n        now = datetime.now(timezone.utc).timestamp()\n\n        if self._nbf is not None:\n            return self._nbf <= now < self._exp\n        else:\n            return now < self._exp\n\n    @property\n    def identity(self) -> str:\n        \n        return self._identity\n\n    @property\n    def issuer(self) -> str:\n        \n        return self._iss\n\n    @property\n    def expected_certificate_subject(self) -> str:\n        \n        if self._federated_issuer is not None:\n            return self._federated_issuer\n\n        return self.issuer\n\n    def __str__(self) -> str:\n        \n        return self._raw_token",
                "class IssuerError(Exception):\n    \n\n    pass",
                "class Issuer:\n    \n\n    def __init__(self, base_url: str) -> None:\n        \n        oidc_config_url = urllib.parse.urljoin(\n            f\"{base_url}/\", \".well-known/openid-configuration\"\n        )\n\n        try:\n            resp: requests.Response = requests.get(oidc_config_url, timeout=30)\n        except (requests.ConnectionError, requests.Timeout) as exc:\n            raise NetworkError from exc\n\n        try:\n            resp.raise_for_status()\n        except requests.HTTPError as http_error:\n            raise IssuerError from http_error\n\n        try:\n            # We don't generally expect this to fail (since the provider should\n            # return a non-success HTTP code which we catch above), but we\n            # check just in case we have a misbehaving OIDC issuer.\n            self.oidc_config = _OpenIDConfiguration.model_validate(resp.json())\n        except ValueError as exc:\n            raise IssuerError(f\"OIDC issuer returned invalid configuration: {exc}\")\n\n    @classmethod\n    def production(cls) -> Issuer:\n        \n        return cls(DEFAULT_OAUTH_ISSUER_URL)\n\n    @classmethod\n    def staging(cls) -> Issuer:\n        \n        return cls(STAGING_OAUTH_ISSUER_URL)\n\n    def identity_token(  # nosec: B107\n        self,\n        client_id: str = \"sigstore\",\n        client_secret: str = \"\",\n        force_oob: bool = False,\n    ) -> IdentityToken:\n        \n\n        # This function and the components that it relies on are based off of:\n        # https://github.com/psteniusubi/python-sample\n\n        from sigstore._internal.oidc.oauth import _OAuthFlow\n\n        code: str\n        with _OAuthFlow(client_id, client_secret, self) as server:\n            # Launch web browser\n            if not force_oob and webbrowser.open(server.base_uri):\n                print(\"Waiting for browser interaction...\", file=sys.stderr)\n            else:\n                server.enable_oob()\n                print(\n                    f\"Go to the following link in a browser:\\n\\n\\t{server.auth_endpoint}\",\n                    file=sys.stderr,\n                )\n\n            if not server.is_oob():\n                # Wait until the redirect server populates the response\n                while server.auth_response is None:\n                    time.sleep(0.1)\n\n                auth_error = server.auth_response.get(\"error\")\n                if auth_error is not None:\n                    raise IdentityError(\n                        f\"Error response from auth endpoint: {auth_error[0]}\"\n                    )\n                code = server.auth_response[\"code\"][0]\n            else:\n                # In the out-of-band case, we wait until the user provides the code\n                code = input(\"Enter verification code: \")\n\n        # Provide code to token endpoint\n        data = {\n            \"grant_type\": \"authorization_code\",\n            \"redirect_uri\": server.redirect_uri,\n            \"code\": code,\n            \"code_verifier\": server.oauth_session.code_verifier,\n        }\n        auth = (\n            client_id,\n            client_secret,\n        )\n        logging.debug(f\"PAYLOAD: data={data}\")\n        try:\n            resp: requests.Response = requests.post(\n                self.oidc_config.token_endpoint,\n                data=data,\n                auth=auth,\n                timeout=30,\n            )\n        except (requests.ConnectionError, requests.Timeout) as exc:\n            raise NetworkError from exc\n\n        try:\n            resp.raise_for_status()\n        except requests.HTTPError as http_error:\n            raise IdentityError(\n                f\"Token request failed with {resp.status_code}\"\n            ) from http_error\n\n        token_json = resp.json()\n        token_error = token_json.get(\"error\")\n        if token_error is not None:\n            raise IdentityError(f\"Error response from token endpoint: {token_error}\")\n\n        return IdentityToken(token_json[\"access_token\"])",
                "class IdentityError(Error):\n    \n\n    @classmethod\n    def raise_from_id(cls, exc: id.IdentityError) -> NoReturn:\n        \n        raise cls(str(exc)) from exc\n\n    def diagnostics(self) -> str:\n        \n        if isinstance(self.__cause__, id.GitHubOidcPermissionCredentialError):\n            return f\n                Insufficient permissions for GitHub Actions workflow.\n\n                The most common reason for this is incorrect\n                configuration of the top-level `permissions` setting of the\n                workflow YAML file. It should be configured like so:\n\n                    permissions:\n                      id-token: write\n\n                Relevant documentation here:\n\n                    https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect#adding-permissions-settings\n\n                Another possible reason is that the workflow run has been\n                triggered by a PR from a forked repository. PRs from forked\n                repositories typically cannot be granted write access.\n\n                Relevant documentation here:\n\n                    https://docs.github.com/en/actions/security-guides/automatic-token-authentication#modifying-the-permissions-for-the-github_token\n\n                Additional context:\n\n                {self.__cause__}\n                \n        else:\n            return f\n                An issue occurred with ambient credential detection.\n\n                Additional context:\n\n                {self}\n            "
            ],
            "classes_docstrings": [
                "\n    Represents a (subset) of the fields provided by an OpenID Connect provider's\n    `.well-known/openid-configuration` response, as defined by OpenID Connect Discovery.\n\n    See: <https://openid.net/specs/openid-connect-discovery-1_0.html>\n    ",
                "An error raised when an identity token is expired.",
                "\n    An OIDC \"identity\", corresponding to an underlying OIDC token with\n    a sensible subject, issuer, and audience for Sigstore purposes.\n    ",
                "\n        Create a new `IdentityToken` from the given OIDC token.\n        ",
                "\n        Returns whether or not this `Identity` is currently within its self-stated validity period.\n\n        NOTE: As noted in `Identity.__init__`, this is not a verifying wrapper;\n        the check here only asserts whether the *unverified* identity's claims\n        are within their validity period.\n        ",
                "\n        Returns this `IdentityToken`'s underlying \"subject\".\n\n        Note that this is **not** always the `sub` claim in the corresponding\n        identity token: depending onm the token's issuer, it may be a *different*\n        claim, such as `email`. This corresponds to the Sigstore ecosystem's\n        behavior, e.g. in each issued certificate's SAN.\n        ",
                "\n        Returns a URL identifying this `IdentityToken`'s issuer.\n        ",
                "\n        Returns a URL identifying the **expected** subject for any Sigstore\n        certificate issued against this identity token.\n\n        The behavior of this field is slightly subtle: for non-federated\n        identity providers (like a token issued directly by Google's IdP) it\n        should be exactly equivalent to `IdentityToken.issuer`. For federated\n        issuers (like Sigstore's own federated IdP) it should be equivalent to\n        the underlying federated issuer's URL, which is kept in an\n        implementation-defined claim.\n\n        This attribute exists so that clients who wish to inspect the expected\n        subject of their certificates can do so without relying on\n        implementation-specific behavior.\n        ",
                "\n        Returns the underlying OIDC token for this identity.\n\n        That this token is secret in nature and **MUST NOT** be disclosed.\n        ",
                "\n    Raised on any communication or format error with an OIDC issuer.\n    ",
                "\n    Represents an OIDC issuer (IdP).\n    ",
                "\n        Create a new `Issuer` from the given base URL.\n\n        This URL is used to locate an OpenID Connect configuration file,\n        which is then used to bootstrap the issuer's state (such\n        as authorization and token endpoints).\n        ",
                "\n        Returns an `Issuer` configured against Sigstore's production-level services.\n        ",
                "\n        Returns an `Issuer` configured against Sigstore's staging-level services.\n        ",
                "\n        Retrieves and returns an `IdentityToken` from the current `Issuer`, via OAuth.\n\n        This function blocks on user interaction.\n\n        The `force_oob` flag controls the kind of flow performed. When `False` (the default),\n        this function attempts to open the user's web browser before falling back to\n        an out-of-band flow. When `True`, the out-of-band flow is always used.\n        ",
                "\n    Wraps `id`'s IdentityError.\n    ",
                "Raises a wrapped IdentityError from the provided `id.IdentityError`.",
                "Returns diagnostics for the error."
            ]
        }
    },
    "sign": {
        "markdown": "[ sigstore](../sigstore.html)\n\n## API Documentation\n\n  * logger\n  * Signer\n    * Signer\n    * sign\n  * SigningContext\n    * SigningContext\n    * production\n    * staging\n    * signer\n  * SigningResult\n    * input_digest\n    * cert_pem\n    * b64_signature\n    * log_entry\n    * to_bundle\n    * model_config\n    * model_fields\n\n[ built with pdoc ](https://pdoc.dev \"pdoc: Python API documentation\ngenerator\")\n\n#  [sigstore](./../sigstore.html).sign\n\nAPI for signing artifacts.\n\nExample:\n\n    \n    \n    from pathlib import Path\n    \n    from [sigstore.sign]() import SigningContext\n    from [sigstore.oidc](oidc.html) import Issuer\n    \n    issuer = Issuer.production()\n    identity = issuer.identity_token()\n    \n    # The artifact to sign\n    artifact = Path(\"foo.txt\")\n    \n    with artifact.open(\"rb\") as file:\n        signing_ctx = SigningContext.production()\n        with signing_ctx.signer(identity, cache=True) as signer:\n            result = signer.sign(file)\n            print(result)\n    \n\nView Source\n    \n\nlogger = <Logger [sigstore.sign]() (INFO)>\n\nclass Signer: View Source\n    \n\nThe primary API for signing operations.\n\nSigner( identity_token:\n[sigstore.oidc.IdentityToken](oidc.html#IdentityToken), signing_ctx:\nSigningContext, cache: bool = True) View Source\n    \n\nCreate a new `Signer`.\n\n`identity_token` is the identity token used to request a signing certificate\nfrom Fulcio.\n\n`signing_ctx` is a `SigningContext` that keeps information about the signing\nconfiguration.\n\n`cache` determines whether the signing certificate and ephemeral private key\nshould be reused (until the certificate expires) to sign different artifacts.\nDefault is `True`.\n\ndef sign(self, input_: IO[bytes]) -> SigningResult: View Source\n    \n\nPublic API for signing blobs\n\nclass SigningContext: View Source\n    \n\nKeep a context between signing operations.\n\nSigningContext( *, fulcio: sigstore._internal.fulcio.client.FulcioClient,\nrekor: sigstore._internal.rekor.client.RekorClient) View Source\n    \n\nCreate a new `SigningContext`.\n\n`fulcio` is a `FulcioClient` capable of connecting to a Fulcio instance and\nreturning signing certificates.\n\n`rekor` is a `RekorClient` capable of connecting to a Rekor instance and\ncreating transparency log entries.\n\n@classmethod\n\ndef production(cls) -> SigningContext: View Source\n    \n\nReturn a `SigningContext` instance configured against Sigstore's production-\nlevel services.\n\n@classmethod\n\ndef staging(cls) -> SigningContext: View Source\n    \n\nReturn a `SignerContext` instance configured against Sigstore's staging-level\nservices.\n\n@contextmanager\n\ndef signer( self, identity_token:\n[sigstore.oidc.IdentityToken](oidc.html#IdentityToken), *, cache: bool = True)\n-> Iterator[Signer]: View Source\n    \n\nA context manager for signing operations.\n\n`identity_token` is the identity token passed to the `Signer` instance and\nused to request a signing certificate from Fulcio.\n\n`cache` determines whether the signing certificate and ephemeral private key\ngenerated by the `Signer` instance should be reused (until the certificate\nexpires) to sign different artifacts. Default is `True`.\n\nclass SigningResult(pydantic.main.BaseModel): View Source\n    \n\nRepresents the artifacts of a signing operation.\n\ninput_digest: [sigstore._utils.HexStr](_utils.html#HexStr)\n\nThe hex-encoded SHA256 digest of the input that was signed for.\n\ncert_pem: [sigstore._utils.PEMCert](_utils.html#PEMCert)\n\nThe PEM-encoded public half of the certificate used for signing.\n\nb64_signature: [sigstore._utils.B64Str](_utils.html#B64Str)\n\nThe base64-encoded signature.\n\nlog_entry: [sigstore.transparency.LogEntry](transparency.html#LogEntry)\n\nA record of the Rekor log entry for the signing operation.\n\ndef to_bundle(self) -> sigstore_protobuf_specs.dev.sigstore.bundle.v1.Bundle:\nView Source\n    \n\nCreates a Sigstore bundle (as defined by Sigstore's protobuf specs) from this\n`SigningResult`.\n\nmodel_config = {}\n\nmodel_fields =  {'input_digest': FieldInfo(annotation=NewType, required=True),\n'cert_pem': FieldInfo(annotation=NewType, required=True), 'b64_signature':\nFieldInfo(annotation=NewType, required=True), 'log_entry':\nFieldInfo(annotation=LogEntry, required=True)}\n\n##### Inherited Members\n\npydantic.main.BaseModel\n\n    BaseModel\n    model_computed_fields\n    model_extra\n    model_fields_set\n    model_construct\n    model_copy\n    model_dump\n    model_dump_json\n    model_json_schema\n    model_parametrized_name\n    model_post_init\n    model_rebuild\n    model_validate\n    model_validate_json\n    model_validate_strings\n    dict\n    json\n    parse_obj\n    parse_raw\n    parse_file\n    from_orm\n    construct\n    copy\n    schema\n    schema_json\n    validate\n    update_forward_refs\n\n",
        "code": [
            {
                "sign.py": "# Copyright 2022 The Sigstore Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nAPI for signing artifacts.\n\nExample:\n\n```python\nfrom pathlib import Path\n\nfrom sigstore.sign import SigningContext\nfrom sigstore.oidc import Issuer\n\nissuer = Issuer.production()\nidentity = issuer.identity_token()\n\n# The artifact to sign\nartifact = Path(\"foo.txt\")\n\nwith artifact.open(\"rb\") as file:\n    signing_ctx = SigningContext.production()\n    with signing_ctx.signer(identity, cache=True) as signer:\n        result = signer.sign(file)\n        print(result)\n```\n\n\nfrom __future__ import annotations\n\nimport base64\nimport logging\nfrom contextlib import contextmanager\nfrom datetime import datetime, timezone\nfrom typing import IO, Iterator, Optional\n\nimport cryptography.x509 as x509\nimport sigstore_rekor_types\nfrom cryptography.hazmat.primitives import hashes, serialization\nfrom cryptography.hazmat.primitives.asymmetric import ec\nfrom cryptography.hazmat.primitives.asymmetric.utils import Prehashed\nfrom cryptography.x509.oid import NameOID\nfrom pydantic import BaseModel\nfrom sigstore_protobuf_specs.dev.sigstore.bundle.v1 import (\n    Bundle,\n    VerificationMaterial,\n)\nfrom sigstore_protobuf_specs.dev.sigstore.common.v1 import (\n    HashAlgorithm,\n    HashOutput,\n    LogId,\n    MessageSignature,\n    X509Certificate,\n    X509CertificateChain,\n)\nfrom sigstore_protobuf_specs.dev.sigstore.rekor.v1 import (\n    Checkpoint,\n    InclusionPromise,\n    InclusionProof,\n    KindVersion,\n    TransparencyLogEntry,\n)\n\nfrom sigstore._internal.fulcio import (\n    ExpiredCertificate,\n    FulcioCertificateSigningResponse,\n    FulcioClient,\n)\nfrom sigstore._internal.rekor.client import RekorClient\nfrom sigstore._internal.sct import verify_sct\nfrom sigstore._internal.tuf import TrustUpdater\nfrom sigstore._utils import B64Str, HexStr, PEMCert, sha256_streaming\nfrom sigstore.oidc import ExpiredIdentity, IdentityToken\nfrom sigstore.transparency import LogEntry\n\nlogger = logging.getLogger(__name__)\n\n\nclass Signer:\n    \n    The primary API for signing operations.\n    \n\n    def __init__(\n        self,\n        identity_token: IdentityToken,\n        signing_ctx: SigningContext,\n        cache: bool = True,\n    ) -> None:\n        \n        Create a new `Signer`.\n\n        `identity_token` is the identity token used to request a signing certificate\n        from Fulcio.\n\n        `signing_ctx` is a `SigningContext` that keeps information about the signing\n        configuration.\n\n        `cache` determines whether the signing certificate and ephemeral private key\n        should be reused (until the certificate expires) to sign different artifacts.\n        Default is `True`.\n        \n        self._identity_token = identity_token\n        self._signing_ctx: SigningContext = signing_ctx\n        self.__cached_private_key: Optional[ec.EllipticCurvePrivateKey] = None\n        self.__cached_signing_certificate: Optional[\n            FulcioCertificateSigningResponse\n        ] = None\n        if cache:\n            logger.debug(\"Generating ephemeral keys...\")\n            self.__cached_private_key = ec.generate_private_key(ec.SECP256R1())\n            logger.debug(\"Requesting ephemeral certificate...\")\n            self.__cached_signing_certificate = self._signing_cert(self._private_key)\n\n    @property\n    def _private_key(self) -> ec.EllipticCurvePrivateKey:\n        Get or generate a signing key.\n        if self.__cached_private_key is None:\n            logger.debug(\"no cached key; generating ephemeral key\")\n            return ec.generate_private_key(ec.SECP256R1())\n        return self.__cached_private_key\n\n    def _signing_cert(\n        self,\n        private_key: ec.EllipticCurvePrivateKey,\n    ) -> FulcioCertificateSigningResponse:\n        Get or request a signing certificate from Fulcio.\n        # If it exists, verify if the current certificate is expired\n        if self.__cached_signing_certificate:\n            not_valid_after = self.__cached_signing_certificate.cert.not_valid_after\n            not_valid_after_tzutc = not_valid_after.replace(tzinfo=timezone.utc)\n            if datetime.now(timezone.utc) > not_valid_after_tzutc:\n                raise ExpiredCertificate\n            return self.__cached_signing_certificate\n\n        else:\n            logger.debug(\"Retrieving signed certificate...\")\n\n            # Build an X.509 Certificiate Signing Request\n            builder = (\n                x509.CertificateSigningRequestBuilder()\n                .subject_name(\n                    x509.Name(\n                        [\n                            x509.NameAttribute(\n                                NameOID.EMAIL_ADDRESS, self._identity_token._identity\n                            ),\n                        ]\n                    )\n                )\n                .add_extension(\n                    x509.BasicConstraints(ca=False, path_length=None),\n                    critical=True,\n                )\n            )\n            certificate_request = builder.sign(private_key, hashes.SHA256())\n\n            certificate_response = self._signing_ctx._fulcio.signing_cert.post(\n                certificate_request, self._identity_token\n            )\n\n            return certificate_response\n\n    def sign(\n        self,\n        input_: IO[bytes],\n    ) -> SigningResult:\n        Public API for signing blobs\n        input_digest = sha256_streaming(input_)\n        private_key = self._private_key\n\n        if not self._identity_token.in_validity_period():\n            raise ExpiredIdentity\n\n        try:\n            certificate_response = self._signing_cert(private_key)\n        except ExpiredCertificate as e:\n            raise e\n\n        # TODO(alex): Retrieve the public key via TUF\n        #\n        # Verify the SCT\n        sct = certificate_response.sct  # noqa\n        cert = certificate_response.cert  # noqa\n        chain = certificate_response.chain\n\n        verify_sct(sct, cert, chain, self._signing_ctx._rekor._ct_keyring)\n\n        logger.debug(\"Successfully verified SCT...\")\n\n        # Sign artifact\n        artifact_signature = private_key.sign(\n            input_digest, ec.ECDSA(Prehashed(hashes.SHA256()))\n        )\n        b64_artifact_signature = B64Str(base64.b64encode(artifact_signature).decode())\n\n        # Prepare inputs\n        b64_cert = base64.b64encode(\n            cert.public_bytes(encoding=serialization.Encoding.PEM)\n        )\n\n        # Create the transparency log entry\n        proposed_entry = sigstore_rekor_types.Hashedrekord(\n            kind=\"hashedrekord\",\n            api_version=\"0.0.1\",\n            spec=sigstore_rekor_types.HashedrekordV001Schema(\n                signature=sigstore_rekor_types.Signature1(\n                    content=b64_artifact_signature,\n                    public_key=sigstore_rekor_types.PublicKey1(\n                        content=b64_cert.decode()\n                    ),\n                ),\n                data=sigstore_rekor_types.Data(\n                    hash=sigstore_rekor_types.Hash(\n                        algorithm=sigstore_rekor_types.Algorithm.SHA256,\n                        value=input_digest.hex(),\n                    )\n                ),\n            ),\n        )\n        entry = self._signing_ctx._rekor.log.entries.post(proposed_entry)\n\n        logger.debug(f\"Transparency log entry created with index: {entry.log_index}\")\n\n        return SigningResult(\n            input_digest=HexStr(input_digest.hex()),\n            cert_pem=PEMCert(\n                cert.public_bytes(encoding=serialization.Encoding.PEM).decode()\n            ),\n            b64_signature=B64Str(b64_artifact_signature),\n            log_entry=entry,\n        )\n\n\nclass SigningContext:\n    \n    Keep a context between signing operations.\n    \n\n    def __init__(\n        self,\n        *,\n        fulcio: FulcioClient,\n        rekor: RekorClient,\n    ):\n        \n        Create a new `SigningContext`.\n\n        `fulcio` is a `FulcioClient` capable of connecting to a Fulcio instance\n        and returning signing certificates.\n\n        `rekor` is a `RekorClient` capable of connecting to a Rekor instance\n        and creating transparency log entries.\n        \n        self._fulcio = fulcio\n        self._rekor = rekor\n\n    @classmethod\n    def production(cls) -> SigningContext:\n        \n        Return a `SigningContext` instance configured against Sigstore's production-level services.\n        \n        updater = TrustUpdater.production()\n        rekor = RekorClient.production(updater)\n        return cls(\n            fulcio=FulcioClient.production(),\n            rekor=rekor,\n        )\n\n    @classmethod\n    def staging(cls) -> SigningContext:\n        \n        Return a `SignerContext` instance configured against Sigstore's staging-level services.\n        \n        updater = TrustUpdater.staging()\n        rekor = RekorClient.staging(updater)\n        return cls(\n            fulcio=FulcioClient.staging(),\n            rekor=rekor,\n        )\n\n    @contextmanager\n    def signer(\n        self, identity_token: IdentityToken, *, cache: bool = True\n    ) -> Iterator[Signer]:\n        \n        A context manager for signing operations.\n\n        `identity_token` is the identity token passed to the `Signer` instance\n        and used to request a signing certificate from Fulcio.\n\n        `cache` determines whether the signing certificate and ephemeral private key\n        generated by the `Signer` instance should be reused (until the certificate expires)\n        to sign different artifacts.\n        Default is `True`.\n        \n        yield Signer(identity_token, self, cache)\n\n\nclass SigningResult(BaseModel):\n    \n    Represents the artifacts of a signing operation.\n    \n\n    input_digest: HexStr\n    \n    The hex-encoded SHA256 digest of the input that was signed for.\n    \n\n    cert_pem: PEMCert\n    \n    The PEM-encoded public half of the certificate used for signing.\n    \n\n    b64_signature: B64Str\n    \n    The base64-encoded signature.\n    \n\n    log_entry: LogEntry\n    \n    A record of the Rekor log entry for the signing operation.\n    \n\n    def to_bundle(self) -> Bundle:\n        \n        Creates a Sigstore bundle (as defined by Sigstore's protobuf specs)\n        from this `SigningResult`.\n        \n\n        # NOTE: We explicitly only include the leaf certificate in the bundle's \"chain\"\n        # here: the specs explicitly forbid the inclusion of the root certificate,\n        # and discourage inclusion of any intermediates (since they're in the root of\n        # trust already).\n        cert = x509.load_pem_x509_certificate(self.cert_pem.encode())\n        cert_der = cert.public_bytes(encoding=serialization.Encoding.DER)\n        chain = X509CertificateChain(certificates=[X509Certificate(raw_bytes=cert_der)])\n\n        inclusion_proof: InclusionProof | None = None\n        if self.log_entry.inclusion_proof is not None:\n            inclusion_proof = InclusionProof(\n                log_index=self.log_entry.inclusion_proof.log_index,\n                root_hash=bytes.fromhex(self.log_entry.inclusion_proof.root_hash),\n                tree_size=self.log_entry.inclusion_proof.tree_size,\n                hashes=[\n                    bytes.fromhex(h) for h in self.log_entry.inclusion_proof.hashes\n                ],\n                checkpoint=Checkpoint(\n                    envelope=self.log_entry.inclusion_proof.checkpoint\n                ),\n            )\n\n        tlog_entry = TransparencyLogEntry(\n            log_index=self.log_entry.log_index,\n            log_id=LogId(key_id=bytes.fromhex(self.log_entry.log_id)),\n            kind_version=KindVersion(kind=\"hashedrekord\", version=\"0.0.1\"),\n            integrated_time=self.log_entry.integrated_time,\n            inclusion_promise=InclusionPromise(\n                signed_entry_timestamp=base64.b64decode(\n                    self.log_entry.inclusion_promise\n                )\n            )\n            if self.log_entry.inclusion_promise\n            else None,\n            inclusion_proof=inclusion_proof,\n            canonicalized_body=base64.b64decode(self.log_entry.body),\n        )\n\n        material = VerificationMaterial(\n            x509_certificate_chain=chain,\n            tlog_entries=[tlog_entry],\n        )\n\n        bundle = Bundle(\n            media_type=\"application/vnd.dev.sigstore.bundle+json;version=0.2\",\n            verification_material=material,\n            message_signature=MessageSignature(\n                message_digest=HashOutput(\n                    algorithm=HashAlgorithm.SHA2_256,\n                    digest=bytes.fromhex(self.input_digest),\n                ),\n                signature=base64.b64decode(self.b64_signature),\n            ),\n        )\n\n        return bundle\n"
            }
        ],
        "code_chunks": {
            "imports": [
                "import base64",
                "import logging",
                "import cryptography.x509 as x509",
                "import sigstore_rekor_types"
            ],
            "functions": [],
            "classes": [
                "class Signer:\n    \n\n    def __init__(\n        self,\n        identity_token: IdentityToken,\n        signing_ctx: SigningContext,\n        cache: bool = True,\n    ) -> None:\n        \n        self._identity_token = identity_token\n        self._signing_ctx: SigningContext = signing_ctx\n        self.__cached_private_key: Optional[ec.EllipticCurvePrivateKey] = None\n        self.__cached_signing_certificate: Optional[\n            FulcioCertificateSigningResponse\n        ] = None\n        if cache:\n            logger.debug(\"Generating ephemeral keys...\")\n            self.__cached_private_key = ec.generate_private_key(ec.SECP256R1())\n            logger.debug(\"Requesting ephemeral certificate...\")\n            self.__cached_signing_certificate = self._signing_cert(self._private_key)\n\n    @property\n    def _private_key(self) -> ec.EllipticCurvePrivateKey:\n        \n        if self.__cached_private_key is None:\n            logger.debug(\"no cached key; generating ephemeral key\")\n            return ec.generate_private_key(ec.SECP256R1())\n        return self.__cached_private_key\n\n    def _signing_cert(\n        self,\n        private_key: ec.EllipticCurvePrivateKey,\n    ) -> FulcioCertificateSigningResponse:\n        \n        # If it exists, verify if the current certificate is expired\n        if self.__cached_signing_certificate:\n            not_valid_after = self.__cached_signing_certificate.cert.not_valid_after\n            not_valid_after_tzutc = not_valid_after.replace(tzinfo=timezone.utc)\n            if datetime.now(timezone.utc) > not_valid_after_tzutc:\n                raise ExpiredCertificate\n            return self.__cached_signing_certificate\n\n        else:\n            logger.debug(\"Retrieving signed certificate...\")\n\n            # Build an X.509 Certificiate Signing Request\n            builder = (\n                x509.CertificateSigningRequestBuilder()\n                .subject_name(\n                    x509.Name(\n                        [\n                            x509.NameAttribute(\n                                NameOID.EMAIL_ADDRESS, self._identity_token._identity\n                            ),\n                        ]\n                    )\n                )\n                .add_extension(\n                    x509.BasicConstraints(ca=False, path_length=None),\n                    critical=True,\n                )\n            )\n            certificate_request = builder.sign(private_key, hashes.SHA256())\n\n            certificate_response = self._signing_ctx._fulcio.signing_cert.post(\n                certificate_request, self._identity_token\n            )\n\n            return certificate_response\n\n    def sign(\n        self,\n        input_: IO[bytes],\n    ) -> SigningResult:\n        \n        input_digest = sha256_streaming(input_)\n        private_key = self._private_key\n\n        if not self._identity_token.in_validity_period():\n            raise ExpiredIdentity\n\n        try:\n            certificate_response = self._signing_cert(private_key)\n        except ExpiredCertificate as e:\n            raise e\n\n        # TODO(alex): Retrieve the public key via TUF\n        #\n        # Verify the SCT\n        sct = certificate_response.sct  # noqa\n        cert = certificate_response.cert  # noqa\n        chain = certificate_response.chain\n\n        verify_sct(sct, cert, chain, self._signing_ctx._rekor._ct_keyring)\n\n        logger.debug(\"Successfully verified SCT...\")\n\n        # Sign artifact\n        artifact_signature = private_key.sign(\n            input_digest, ec.ECDSA(Prehashed(hashes.SHA256()))\n        )\n        b64_artifact_signature = B64Str(base64.b64encode(artifact_signature).decode())\n\n        # Prepare inputs\n        b64_cert = base64.b64encode(\n            cert.public_bytes(encoding=serialization.Encoding.PEM)\n        )\n\n        # Create the transparency log entry\n        proposed_entry = sigstore_rekor_types.Hashedrekord(\n            kind=\"hashedrekord\",\n            api_version=\"0.0.1\",\n            spec=sigstore_rekor_types.HashedrekordV001Schema(\n                signature=sigstore_rekor_types.Signature1(\n                    content=b64_artifact_signature,\n                    public_key=sigstore_rekor_types.PublicKey1(\n                        content=b64_cert.decode()\n                    ),\n                ),\n                data=sigstore_rekor_types.Data(\n                    hash=sigstore_rekor_types.Hash(\n                        algorithm=sigstore_rekor_types.Algorithm.SHA256,\n                        value=input_digest.hex(),\n                    )\n                ),\n            ),\n        )\n        entry = self._signing_ctx._rekor.log.entries.post(proposed_entry)\n\n        logger.debug(f\"Transparency log entry created with index: {entry.log_index}\")\n\n        return SigningResult(\n            input_digest=HexStr(input_digest.hex()),\n            cert_pem=PEMCert(\n                cert.public_bytes(encoding=serialization.Encoding.PEM).decode()\n            ),\n            b64_signature=B64Str(b64_artifact_signature),\n            log_entry=entry,\n        )",
                "class SigningContext:\n    \n\n    def __init__(\n        self,\n        *,\n        fulcio: FulcioClient,\n        rekor: RekorClient,\n    ):\n        \n        self._fulcio = fulcio\n        self._rekor = rekor\n\n    @classmethod\n    def production(cls) -> SigningContext:\n        \n        updater = TrustUpdater.production()\n        rekor = RekorClient.production(updater)\n        return cls(\n            fulcio=FulcioClient.production(),\n            rekor=rekor,\n        )\n\n    @classmethod\n    def staging(cls) -> SigningContext:\n        \n        updater = TrustUpdater.staging()\n        rekor = RekorClient.staging(updater)\n        return cls(\n            fulcio=FulcioClient.staging(),\n            rekor=rekor,\n        )\n\n    @contextmanager\n    def signer(\n        self, identity_token: IdentityToken, *, cache: bool = True\n    ) -> Iterator[Signer]:\n        \n        yield Signer(identity_token, self, cache)",
                "class SigningResult(BaseModel):\n    \n\n    input_digest: HexStr\n    \n\n    cert_pem: PEMCert\n    \n\n    b64_signature: B64Str\n    \n\n    log_entry: LogEntry\n    \n\n    def to_bundle(self) -> Bundle:\n        \n\n        # NOTE: We explicitly only include the leaf certificate in the bundle's \"chain\"\n        # here: the specs explicitly forbid the inclusion of the root certificate,\n        # and discourage inclusion of any intermediates (since they're in the root of\n        # trust already).\n        cert = x509.load_pem_x509_certificate(self.cert_pem.encode())\n        cert_der = cert.public_bytes(encoding=serialization.Encoding.DER)\n        chain = X509CertificateChain(certificates=[X509Certificate(raw_bytes=cert_der)])\n\n        inclusion_proof: InclusionProof | None = None\n        if self.log_entry.inclusion_proof is not None:\n            inclusion_proof = InclusionProof(\n                log_index=self.log_entry.inclusion_proof.log_index,\n                root_hash=bytes.fromhex(self.log_entry.inclusion_proof.root_hash),\n                tree_size=self.log_entry.inclusion_proof.tree_size,\n                hashes=[\n                    bytes.fromhex(h) for h in self.log_entry.inclusion_proof.hashes\n                ],\n                checkpoint=Checkpoint(\n                    envelope=self.log_entry.inclusion_proof.checkpoint\n                ),\n            )\n\n        tlog_entry = TransparencyLogEntry(\n            log_index=self.log_entry.log_index,\n            log_id=LogId(key_id=bytes.fromhex(self.log_entry.log_id)),\n            kind_version=KindVersion(kind=\"hashedrekord\", version=\"0.0.1\"),\n            integrated_time=self.log_entry.integrated_time,\n            inclusion_promise=InclusionPromise(\n                signed_entry_timestamp=base64.b64decode(\n                    self.log_entry.inclusion_promise\n                )\n            )\n            if self.log_entry.inclusion_promise\n            else None,\n            inclusion_proof=inclusion_proof,\n            canonicalized_body=base64.b64decode(self.log_entry.body),\n        )\n\n        material = VerificationMaterial(\n            x509_certificate_chain=chain,\n            tlog_entries=[tlog_entry],\n        )\n\n        bundle = Bundle(\n            media_type=\"application/vnd.dev.sigstore.bundle+json;version=0.2\",\n            verification_material=material,\n            message_signature=MessageSignature(\n                message_digest=HashOutput(\n                    algorithm=HashAlgorithm.SHA2_256,\n                    digest=bytes.fromhex(self.input_digest),\n                ),\n                signature=base64.b64decode(self.b64_signature),\n            ),\n        )\n\n        return bundle"
            ],
            "documentation": [
                "\nAPI for signing artifacts.\n\nExample:\n\n```python\nfrom pathlib import Path\n\nfrom sigstore.sign import SigningContext\nfrom sigstore.oidc import Issuer\n\nissuer = Issuer.production()\nidentity = issuer.identity_token()\n\n# The artifact to sign\nartifact = Path(\"foo.txt\")\n\nwith artifact.open(\"rb\") as file:\n    signing_ctx = SigningContext.production()\n    with signing_ctx.signer(identity, cache=True) as signer:\n        result = signer.sign(file)\n        print(result)\n```\n"
            ],
            "other": [
                "# Copyright 2022 The Sigstore Authors",
                "#",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");",
                "# you may not use this file except in compliance with the License.",
                "# You may obtain a copy of the License at",
                "#",
                "#      http://www.apache.org/licenses/LICENSE-2.0",
                "#",
                "# Unless required by applicable law or agreed to in writing, software",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "# See the License for the specific language governing permissions and",
                "# limitations under the License.",
                "from __future__ import annotations",
                "from contextlib import contextmanager",
                "from datetime import datetime, timezone",
                "from typing import IO, Iterator, Optional",
                "from cryptography.hazmat.primitives import hashes, serialization",
                "from cryptography.hazmat.primitives.asymmetric import ec",
                "from cryptography.hazmat.primitives.asymmetric.utils import Prehashed",
                "from cryptography.x509.oid import NameOID",
                "from pydantic import BaseModel",
                "from sigstore_protobuf_specs.dev.sigstore.bundle.v1 import (\n    Bundle,\n    VerificationMaterial,\n)",
                "from sigstore_protobuf_specs.dev.sigstore.common.v1 import (\n    HashAlgorithm,\n    HashOutput,\n    LogId,\n    MessageSignature,\n    X509Certificate,\n    X509CertificateChain,\n)",
                "from sigstore_protobuf_specs.dev.sigstore.rekor.v1 import (\n    Checkpoint,\n    InclusionPromise,\n    InclusionProof,\n    KindVersion,\n    TransparencyLogEntry,\n)",
                "from sigstore._internal.fulcio import (\n    ExpiredCertificate,\n    FulcioCertificateSigningResponse,\n    FulcioClient,\n)",
                "from sigstore._internal.rekor.client import RekorClient",
                "from sigstore._internal.sct import verify_sct",
                "from sigstore._internal.tuf import TrustUpdater",
                "from sigstore._utils import B64Str, HexStr, PEMCert, sha256_streaming",
                "from sigstore.oidc import ExpiredIdentity, IdentityToken",
                "from sigstore.transparency import LogEntry",
                "logger = logging.getLogger(__name__)"
            ],
            "functions_code": [],
            "functions_docstrings": [],
            "classes_code": [
                "class Signer:\n    \n\n    def __init__(\n        self,\n        identity_token: IdentityToken,\n        signing_ctx: SigningContext,\n        cache: bool = True,\n    ) -> None:\n        \n        self._identity_token = identity_token\n        self._signing_ctx: SigningContext = signing_ctx\n        self.__cached_private_key: Optional[ec.EllipticCurvePrivateKey] = None\n        self.__cached_signing_certificate: Optional[\n            FulcioCertificateSigningResponse\n        ] = None\n        if cache:\n            logger.debug(\"Generating ephemeral keys...\")\n            self.__cached_private_key = ec.generate_private_key(ec.SECP256R1())\n            logger.debug(\"Requesting ephemeral certificate...\")\n            self.__cached_signing_certificate = self._signing_cert(self._private_key)\n\n    @property\n    def _private_key(self) -> ec.EllipticCurvePrivateKey:\n        \n        if self.__cached_private_key is None:\n            logger.debug(\"no cached key; generating ephemeral key\")\n            return ec.generate_private_key(ec.SECP256R1())\n        return self.__cached_private_key\n\n    def _signing_cert(\n        self,\n        private_key: ec.EllipticCurvePrivateKey,\n    ) -> FulcioCertificateSigningResponse:\n        \n        # If it exists, verify if the current certificate is expired\n        if self.__cached_signing_certificate:\n            not_valid_after = self.__cached_signing_certificate.cert.not_valid_after\n            not_valid_after_tzutc = not_valid_after.replace(tzinfo=timezone.utc)\n            if datetime.now(timezone.utc) > not_valid_after_tzutc:\n                raise ExpiredCertificate\n            return self.__cached_signing_certificate\n\n        else:\n            logger.debug(\"Retrieving signed certificate...\")\n\n            # Build an X.509 Certificiate Signing Request\n            builder = (\n                x509.CertificateSigningRequestBuilder()\n                .subject_name(\n                    x509.Name(\n                        [\n                            x509.NameAttribute(\n                                NameOID.EMAIL_ADDRESS, self._identity_token._identity\n                            ),\n                        ]\n                    )\n                )\n                .add_extension(\n                    x509.BasicConstraints(ca=False, path_length=None),\n                    critical=True,\n                )\n            )\n            certificate_request = builder.sign(private_key, hashes.SHA256())\n\n            certificate_response = self._signing_ctx._fulcio.signing_cert.post(\n                certificate_request, self._identity_token\n            )\n\n            return certificate_response\n\n    def sign(\n        self,\n        input_: IO[bytes],\n    ) -> SigningResult:\n        \n        input_digest = sha256_streaming(input_)\n        private_key = self._private_key\n\n        if not self._identity_token.in_validity_period():\n            raise ExpiredIdentity\n\n        try:\n            certificate_response = self._signing_cert(private_key)\n        except ExpiredCertificate as e:\n            raise e\n\n        # TODO(alex): Retrieve the public key via TUF\n        #\n        # Verify the SCT\n        sct = certificate_response.sct  # noqa\n        cert = certificate_response.cert  # noqa\n        chain = certificate_response.chain\n\n        verify_sct(sct, cert, chain, self._signing_ctx._rekor._ct_keyring)\n\n        logger.debug(\"Successfully verified SCT...\")\n\n        # Sign artifact\n        artifact_signature = private_key.sign(\n            input_digest, ec.ECDSA(Prehashed(hashes.SHA256()))\n        )\n        b64_artifact_signature = B64Str(base64.b64encode(artifact_signature).decode())\n\n        # Prepare inputs\n        b64_cert = base64.b64encode(\n            cert.public_bytes(encoding=serialization.Encoding.PEM)\n        )\n\n        # Create the transparency log entry\n        proposed_entry = sigstore_rekor_types.Hashedrekord(\n            kind=\"hashedrekord\",\n            api_version=\"0.0.1\",\n            spec=sigstore_rekor_types.HashedrekordV001Schema(\n                signature=sigstore_rekor_types.Signature1(\n                    content=b64_artifact_signature,\n                    public_key=sigstore_rekor_types.PublicKey1(\n                        content=b64_cert.decode()\n                    ),\n                ),\n                data=sigstore_rekor_types.Data(\n                    hash=sigstore_rekor_types.Hash(\n                        algorithm=sigstore_rekor_types.Algorithm.SHA256,\n                        value=input_digest.hex(),\n                    )\n                ),\n            ),\n        )\n        entry = self._signing_ctx._rekor.log.entries.post(proposed_entry)\n\n        logger.debug(f\"Transparency log entry created with index: {entry.log_index}\")\n\n        return SigningResult(\n            input_digest=HexStr(input_digest.hex()),\n            cert_pem=PEMCert(\n                cert.public_bytes(encoding=serialization.Encoding.PEM).decode()\n            ),\n            b64_signature=B64Str(b64_artifact_signature),\n            log_entry=entry,\n        )",
                "class SigningContext:\n    \n\n    def __init__(\n        self,\n        *,\n        fulcio: FulcioClient,\n        rekor: RekorClient,\n    ):\n        \n        self._fulcio = fulcio\n        self._rekor = rekor\n\n    @classmethod\n    def production(cls) -> SigningContext:\n        \n        updater = TrustUpdater.production()\n        rekor = RekorClient.production(updater)\n        return cls(\n            fulcio=FulcioClient.production(),\n            rekor=rekor,\n        )\n\n    @classmethod\n    def staging(cls) -> SigningContext:\n        \n        updater = TrustUpdater.staging()\n        rekor = RekorClient.staging(updater)\n        return cls(\n            fulcio=FulcioClient.staging(),\n            rekor=rekor,\n        )\n\n    @contextmanager\n    def signer(\n        self, identity_token: IdentityToken, *, cache: bool = True\n    ) -> Iterator[Signer]:\n        \n        yield Signer(identity_token, self, cache)",
                "class SigningResult(BaseModel):\n    \n\n    input_digest: HexStr\n    \n\n    cert_pem: PEMCert\n    \n\n    b64_signature: B64Str\n    \n\n    log_entry: LogEntry\n    \n\n    def to_bundle(self) -> Bundle:\n        \n\n        # NOTE: We explicitly only include the leaf certificate in the bundle's \"chain\"\n        # here: the specs explicitly forbid the inclusion of the root certificate,\n        # and discourage inclusion of any intermediates (since they're in the root of\n        # trust already).\n        cert = x509.load_pem_x509_certificate(self.cert_pem.encode())\n        cert_der = cert.public_bytes(encoding=serialization.Encoding.DER)\n        chain = X509CertificateChain(certificates=[X509Certificate(raw_bytes=cert_der)])\n\n        inclusion_proof: InclusionProof | None = None\n        if self.log_entry.inclusion_proof is not None:\n            inclusion_proof = InclusionProof(\n                log_index=self.log_entry.inclusion_proof.log_index,\n                root_hash=bytes.fromhex(self.log_entry.inclusion_proof.root_hash),\n                tree_size=self.log_entry.inclusion_proof.tree_size,\n                hashes=[\n                    bytes.fromhex(h) for h in self.log_entry.inclusion_proof.hashes\n                ],\n                checkpoint=Checkpoint(\n                    envelope=self.log_entry.inclusion_proof.checkpoint\n                ),\n            )\n\n        tlog_entry = TransparencyLogEntry(\n            log_index=self.log_entry.log_index,\n            log_id=LogId(key_id=bytes.fromhex(self.log_entry.log_id)),\n            kind_version=KindVersion(kind=\"hashedrekord\", version=\"0.0.1\"),\n            integrated_time=self.log_entry.integrated_time,\n            inclusion_promise=InclusionPromise(\n                signed_entry_timestamp=base64.b64decode(\n                    self.log_entry.inclusion_promise\n                )\n            )\n            if self.log_entry.inclusion_promise\n            else None,\n            inclusion_proof=inclusion_proof,\n            canonicalized_body=base64.b64decode(self.log_entry.body),\n        )\n\n        material = VerificationMaterial(\n            x509_certificate_chain=chain,\n            tlog_entries=[tlog_entry],\n        )\n\n        bundle = Bundle(\n            media_type=\"application/vnd.dev.sigstore.bundle+json;version=0.2\",\n            verification_material=material,\n            message_signature=MessageSignature(\n                message_digest=HashOutput(\n                    algorithm=HashAlgorithm.SHA2_256,\n                    digest=bytes.fromhex(self.input_digest),\n                ),\n                signature=base64.b64decode(self.b64_signature),\n            ),\n        )\n\n        return bundle"
            ],
            "classes_docstrings": [
                "\n    The primary API for signing operations.\n    ",
                "\n        Create a new `Signer`.\n\n        `identity_token` is the identity token used to request a signing certificate\n        from Fulcio.\n\n        `signing_ctx` is a `SigningContext` that keeps information about the signing\n        configuration.\n\n        `cache` determines whether the signing certificate and ephemeral private key\n        should be reused (until the certificate expires) to sign different artifacts.\n        Default is `True`.\n        ",
                "Get or generate a signing key.",
                "Get or request a signing certificate from Fulcio.",
                "Public API for signing blobs",
                "\n    Keep a context between signing operations.\n    ",
                "\n        Create a new `SigningContext`.\n\n        `fulcio` is a `FulcioClient` capable of connecting to a Fulcio instance\n        and returning signing certificates.\n\n        `rekor` is a `RekorClient` capable of connecting to a Rekor instance\n        and creating transparency log entries.\n        ",
                "\n        Return a `SigningContext` instance configured against Sigstore's production-level services.\n        ",
                "\n        Return a `SignerContext` instance configured against Sigstore's staging-level services.\n        ",
                "\n        A context manager for signing operations.\n\n        `identity_token` is the identity token passed to the `Signer` instance\n        and used to request a signing certificate from Fulcio.\n\n        `cache` determines whether the signing certificate and ephemeral private key\n        generated by the `Signer` instance should be reused (until the certificate expires)\n        to sign different artifacts.\n        Default is `True`.\n        ",
                "\n    Represents the artifacts of a signing operation.\n    ",
                "\n    The hex-encoded SHA256 digest of the input that was signed for.\n    ",
                "\n    The PEM-encoded public half of the certificate used for signing.\n    ",
                "\n    The base64-encoded signature.\n    ",
                "\n    A record of the Rekor log entry for the signing operation.\n    ",
                "\n        Creates a Sigstore bundle (as defined by Sigstore's protobuf specs)\n        from this `SigningResult`.\n        "
            ]
        }
    },
    "transparency": {
        "markdown": "[ sigstore](../sigstore.html)\n\n## API Documentation\n\n  * LogInclusionProof\n    * model_config\n    * checkpoint\n    * hashes\n    * log_index\n    * root_hash\n    * tree_size\n    * model_fields\n  * LogEntry\n    * LogEntry\n    * uuid\n    * body\n    * integrated_time\n    * log_id\n    * log_index\n    * inclusion_proof\n    * inclusion_promise\n    * encode_canonical\n\n[ built with pdoc ](https://pdoc.dev \"pdoc: Python API documentation\ngenerator\")\n\n#  [sigstore](./../sigstore.html).transparency\n\nTransparency log data structures.\n\nView Source\n    \n\nclass LogInclusionProof(pydantic.main.BaseModel): View Source\n    \n\nRepresents an inclusion proof for a transparency log entry.\n\nmodel_config = {'populate_by_name': True}\n\ncheckpoint: Annotated[str, Strict(strict=True)]\n\nhashes: List[Annotated[str, Strict(strict=True)]]\n\nlog_index: Annotated[int, Strict(strict=True)]\n\nroot_hash: Annotated[str, Strict(strict=True)]\n\ntree_size: Annotated[int, Strict(strict=True)]\n\nmodel_fields =  {'checkpoint': FieldInfo(annotation=str, required=True,\nalias='checkpoint', alias_priority=2, metadata=[Strict(strict=True)]),\n'hashes': FieldInfo(annotation=List[Annotated[str, Strict(strict=True)]],\nrequired=True, alias='hashes', alias_priority=2), 'log_index':\nFieldInfo(annotation=int, required=True, alias='logIndex', alias_priority=2,\nmetadata=[Strict(strict=True)]), 'root_hash': FieldInfo(annotation=str,\nrequired=True, alias='rootHash', alias_priority=2,\nmetadata=[Strict(strict=True)]), 'tree_size': FieldInfo(annotation=int,\nrequired=True, alias='treeSize', alias_priority=2,\nmetadata=[Strict(strict=True)])}\n\n##### Inherited Members\n\npydantic.main.BaseModel\n\n    BaseModel\n    model_computed_fields\n    model_extra\n    model_fields_set\n    model_construct\n    model_copy\n    model_dump\n    model_dump_json\n    model_json_schema\n    model_parametrized_name\n    model_post_init\n    model_rebuild\n    model_validate\n    model_validate_json\n    model_validate_strings\n    dict\n    json\n    parse_obj\n    parse_raw\n    parse_file\n    from_orm\n    construct\n    copy\n    schema\n    schema_json\n    validate\n    update_forward_refs\n\n@dataclass(frozen=True)\n\nclass LogEntry: View Source\n    \n\nRepresents a transparency log entry.\n\nLog entries are retrieved from the transparency log after signing or\nverification events, or loaded from \"Sigstore\" bundles provided by the user.\n\nThis representation allows for either a missing inclusion promise or a missing\ninclusion proof, but not both: attempting to construct a `LogEntry` without at\nleast one will fail.\n\nLogEntry(*args: Any, **kwargs: Any) View Source\n    \n\nuuid: Optional[str]\n\nThis entry's unique ID in the log instance it was retrieved from.\n\nFor sharded log deployments, IDs are unique per-shard.\n\nNot present for `LogEntry` instances loaded from Sigstore bundles.\n\nbody: [sigstore._utils.B64Str](_utils.html#B64Str)\n\nThe base64-encoded body of the transparency log entry.\n\nintegrated_time: int\n\nThe UNIX time at which this entry was integrated into the transparency log.\n\nlog_id: str\n\nThe log's ID (as the SHA256 hash of the DER-encoded public key for the log at\nthe time of entry inclusion).\n\nlog_index: int\n\nThe index of this entry within the log.\n\ninclusion_proof: Optional[LogInclusionProof]\n\nAn inclusion proof for this log entry, if present.\n\ninclusion_promise: Optional[[sigstore._utils.B64Str](_utils.html#B64Str)]\n\nAn inclusion promise for this log entry, if present.\n\nInternally, this is a base64-encoded Signed Entry Timestamp (SET) for this log\nentry.\n\ndef encode_canonical(self) -> bytes: View Source\n    \n\nReturns a canonicalized JSON (RFC 8785) representation of the transparency log\nentry.\n\nThis encoded representation is suitable for verification against the Signed\nEntry Timestamp.\n\n",
        "code": [
            {
                "transparency.py": "# Copyright 2022 The Sigstore Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nTransparency log data structures.\n\n\nfrom __future__ import annotations\n\nfrom typing import Any, List, Optional\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    StrictInt,\n    StrictStr,\n    ValidationInfo,\n    field_validator,\n)\nfrom pydantic.dataclasses import dataclass\nfrom securesystemslib.formats import encode_canonical\n\nfrom sigstore._utils import B64Str\n\n\nclass LogInclusionProof(BaseModel):\n    \n    Represents an inclusion proof for a transparency log entry.\n    \n\n    model_config = ConfigDict(populate_by_name=True)\n\n    checkpoint: StrictStr = Field(..., alias=\"checkpoint\")\n    hashes: List[StrictStr] = Field(..., alias=\"hashes\")\n    log_index: StrictInt = Field(..., alias=\"logIndex\")\n    root_hash: StrictStr = Field(..., alias=\"rootHash\")\n    tree_size: StrictInt = Field(..., alias=\"treeSize\")\n\n    @field_validator(\"log_index\")\n    def _log_index_positive(cls, v: int) -> int:\n        if v < 0:\n            raise ValueError(f\"Inclusion proof has invalid log index: {v} < 0\")\n        return v\n\n    @field_validator(\"tree_size\")\n    def _tree_size_positive(cls, v: int) -> int:\n        if v < 0:\n            raise ValueError(f\"Inclusion proof has invalid tree size: {v} < 0\")\n        return v\n\n    @field_validator(\"tree_size\")\n    def _log_index_within_tree_size(\n        cls, v: int, info: ValidationInfo, **kwargs: Any\n    ) -> int:\n        if \"log_index\" in info.data and v <= info.data[\"log_index\"]:\n            raise ValueError(\n                \"Inclusion proof has log index greater than or equal to tree size: \"\n                f\"{v} <= {info.data['log_index']}\"\n            )\n        return v\n\n\n@dataclass(frozen=True)\nclass LogEntry:\n    \n    Represents a transparency log entry.\n\n    Log entries are retrieved from the transparency log after signing or verification events,\n    or loaded from \"Sigstore\" bundles provided by the user.\n\n    This representation allows for either a missing inclusion promise or a missing\n    inclusion proof, but not both: attempting to construct a `LogEntry` without\n    at least one will fail.\n    \n\n    uuid: Optional[str]\n    \n    This entry's unique ID in the log instance it was retrieved from.\n\n    For sharded log deployments, IDs are unique per-shard.\n\n    Not present for `LogEntry` instances loaded from Sigstore bundles.\n    \n\n    body: B64Str\n    \n    The base64-encoded body of the transparency log entry.\n    \n\n    integrated_time: int\n    \n    The UNIX time at which this entry was integrated into the transparency log.\n    \n\n    log_id: str\n    \n    The log's ID (as the SHA256 hash of the DER-encoded public key for the log\n    at the time of entry inclusion).\n    \n\n    log_index: int\n    \n    The index of this entry within the log.\n    \n\n    inclusion_proof: Optional[LogInclusionProof]\n    \n    An inclusion proof for this log entry, if present.\n    \n\n    inclusion_promise: Optional[B64Str]\n    \n    An inclusion promise for this log entry, if present.\n\n    Internally, this is a base64-encoded Signed Entry Timestamp (SET) for this\n    log entry.\n    \n\n    def __post_init__(self) -> None:\n        \n        Invariant preservation.\n        \n\n        # An inclusion proof isn't considered present unless its checkpoint\n        # is also present.\n        has_inclusion_proof = (\n            self.inclusion_proof is not None and self.inclusion_proof.checkpoint\n        )\n        if not has_inclusion_proof and self.inclusion_promise is None:\n            raise ValueError(\"Log entry must have either inclusion proof or promise\")\n\n    @classmethod\n    def _from_response(cls, dict_: dict[str, Any]) -> LogEntry:\n        \n        Create a new `LogEntry` from the given API response.\n        \n\n        # Assumes we only get one entry back\n        entries = list(dict_.items())\n        if len(entries) != 1:\n            raise ValueError(\"Received multiple entries in response\")\n\n        uuid, entry = entries[0]\n        return LogEntry(\n            uuid=uuid,\n            body=entry[\"body\"],\n            integrated_time=entry[\"integratedTime\"],\n            log_id=entry[\"logID\"],\n            log_index=entry[\"logIndex\"],\n            inclusion_proof=LogInclusionProof.model_validate(\n                entry[\"verification\"][\"inclusionProof\"]\n            ),\n            inclusion_promise=entry[\"verification\"][\"signedEntryTimestamp\"],\n        )\n\n    def encode_canonical(self) -> bytes:\n        \n        Returns a canonicalized JSON (RFC 8785) representation of the transparency log entry.\n\n        This encoded representation is suitable for verification against\n        the Signed Entry Timestamp.\n        \n        payload = {\n            \"body\": self.body,\n            \"integratedTime\": self.integrated_time,\n            \"logID\": self.log_id,\n            \"logIndex\": self.log_index,\n        }\n\n        return encode_canonical(payload).encode()  # type: ignore\n"
            }
        ],
        "code_chunks": {
            "imports": [],
            "functions": [],
            "classes": [
                "class LogInclusionProof(BaseModel):\n    \n\n    model_config = ConfigDict(populate_by_name=True)\n\n    checkpoint: StrictStr = Field(..., alias=\"checkpoint\")\n    hashes: List[StrictStr] = Field(..., alias=\"hashes\")\n    log_index: StrictInt = Field(..., alias=\"logIndex\")\n    root_hash: StrictStr = Field(..., alias=\"rootHash\")\n    tree_size: StrictInt = Field(..., alias=\"treeSize\")\n\n    @field_validator(\"log_index\")\n    def _log_index_positive(cls, v: int) -> int:\n        if v < 0:\n            raise ValueError(f\"Inclusion proof has invalid log index: {v} < 0\")\n        return v\n\n    @field_validator(\"tree_size\")\n    def _tree_size_positive(cls, v: int) -> int:\n        if v < 0:\n            raise ValueError(f\"Inclusion proof has invalid tree size: {v} < 0\")\n        return v\n\n    @field_validator(\"tree_size\")\n    def _log_index_within_tree_size(\n        cls, v: int, info: ValidationInfo, **kwargs: Any\n    ) -> int:\n        if \"log_index\" in info.data and v <= info.data[\"log_index\"]:\n            raise ValueError(\n                \"Inclusion proof has log index greater than or equal to tree size: \"\n                f\"{v} <= {info.data['log_index']}\"\n            )\n        return v"
            ],
            "documentation": [
                "\nTransparency log data structures.\n"
            ],
            "other": [
                "# Copyright 2022 The Sigstore Authors",
                "#",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");",
                "# you may not use this file except in compliance with the License.",
                "# You may obtain a copy of the License at",
                "#",
                "#      http://www.apache.org/licenses/LICENSE-2.0",
                "#",
                "# Unless required by applicable law or agreed to in writing, software",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "# See the License for the specific language governing permissions and",
                "# limitations under the License.",
                "from __future__ import annotations",
                "from typing import Any, List, Optional",
                "from pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    StrictInt,\n    StrictStr,\n    ValidationInfo,\n    field_validator,\n)",
                "from pydantic.dataclasses import dataclass",
                "from securesystemslib.formats import encode_canonical",
                "from sigstore._utils import B64Str",
                "@dataclass(frozen=True)\nclass LogEntry:\n    \n    Represents a transparency log entry.\n\n    Log entries are retrieved from the transparency log after signing or verification events,\n    or loaded from \"Sigstore\" bundles provided by the user.\n\n    This representation allows for either a missing inclusion promise or a missing\n    inclusion proof, but not both: attempting to construct a `LogEntry` without\n    at least one will fail.\n    \n\n    uuid: Optional[str]\n    \n    This entry's unique ID in the log instance it was retrieved from.\n\n    For sharded log deployments, IDs are unique per-shard.\n\n    Not present for `LogEntry` instances loaded from Sigstore bundles.\n    \n\n    body: B64Str\n    \n    The base64-encoded body of the transparency log entry.\n    \n\n    integrated_time: int\n    \n    The UNIX time at which this entry was integrated into the transparency log.\n    \n\n    log_id: str\n    \n    The log's ID (as the SHA256 hash of the DER-encoded public key for the log\n    at the time of entry inclusion).\n    \n\n    log_index: int\n    \n    The index of this entry within the log.\n    \n\n    inclusion_proof: Optional[LogInclusionProof]\n    \n    An inclusion proof for this log entry, if present.\n    \n\n    inclusion_promise: Optional[B64Str]\n    \n    An inclusion promise for this log entry, if present.\n\n    Internally, this is a base64-encoded Signed Entry Timestamp (SET) for this\n    log entry.\n    \n\n    def __post_init__(self) -> None:\n        \n        Invariant preservation.\n        \n\n        # An inclusion proof isn't considered present unless its checkpoint\n        # is also present.\n        has_inclusion_proof = (\n            self.inclusion_proof is not None and self.inclusion_proof.checkpoint\n        )\n        if not has_inclusion_proof and self.inclusion_promise is None:\n            raise ValueError(\"Log entry must have either inclusion proof or promise\")\n\n    @classmethod\n    def _from_response(cls, dict_: dict[str, Any]) -> LogEntry:\n        \n        Create a new `LogEntry` from the given API response.\n        \n\n        # Assumes we only get one entry back\n        entries = list(dict_.items())\n        if len(entries) != 1:\n            raise ValueError(\"Received multiple entries in response\")\n\n        uuid, entry = entries[0]\n        return LogEntry(\n            uuid=uuid,\n            body=entry[\"body\"],\n            integrated_time=entry[\"integratedTime\"],\n            log_id=entry[\"logID\"],\n            log_index=entry[\"logIndex\"],\n            inclusion_proof=LogInclusionProof.model_validate(\n                entry[\"verification\"][\"inclusionProof\"]\n            ),\n            inclusion_promise=entry[\"verification\"][\"signedEntryTimestamp\"],\n        )\n\n    def encode_canonical(self) -> bytes:\n        \n        Returns a canonicalized JSON (RFC 8785) representation of the transparency log entry.\n\n        This encoded representation is suitable for verification against\n        the Signed Entry Timestamp.\n        \n        payload = {\n            \"body\": self.body,\n            \"integratedTime\": self.integrated_time,\n            \"logID\": self.log_id,\n            \"logIndex\": self.log_index,\n        }\n\n        return encode_canonical(payload).encode()  # type: ignore"
            ],
            "functions_code": [],
            "functions_docstrings": [],
            "classes_code": [
                "class LogInclusionProof(BaseModel):\n    \n\n    model_config = ConfigDict(populate_by_name=True)\n\n    checkpoint: StrictStr = Field(..., alias=\"checkpoint\")\n    hashes: List[StrictStr] = Field(..., alias=\"hashes\")\n    log_index: StrictInt = Field(..., alias=\"logIndex\")\n    root_hash: StrictStr = Field(..., alias=\"rootHash\")\n    tree_size: StrictInt = Field(..., alias=\"treeSize\")\n\n    @field_validator(\"log_index\")\n    def _log_index_positive(cls, v: int) -> int:\n        if v < 0:\n            raise ValueError(f\"Inclusion proof has invalid log index: {v} < 0\")\n        return v\n\n    @field_validator(\"tree_size\")\n    def _tree_size_positive(cls, v: int) -> int:\n        if v < 0:\n            raise ValueError(f\"Inclusion proof has invalid tree size: {v} < 0\")\n        return v\n\n    @field_validator(\"tree_size\")\n    def _log_index_within_tree_size(\n        cls, v: int, info: ValidationInfo, **kwargs: Any\n    ) -> int:\n        if \"log_index\" in info.data and v <= info.data[\"log_index\"]:\n            raise ValueError(\n                \"Inclusion proof has log index greater than or equal to tree size: \"\n                f\"{v} <= {info.data['log_index']}\"\n            )\n        return v"
            ],
            "classes_docstrings": [
                "\n    Represents an inclusion proof for a transparency log entry.\n    "
            ]
        }
    },
    "verify_models": {
        "markdown": "",
        "code": [
            {
                "verify/models.py": "# Copyright 2022 The Sigstore Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nCommon (base) models for the verification APIs.\n\n\nfrom __future__ import annotations\n\nimport base64\nimport json\nimport logging\nfrom dataclasses import dataclass\nfrom textwrap import dedent\nfrom typing import IO\n\nimport sigstore_rekor_types\nfrom cryptography.hazmat.primitives.serialization import Encoding\nfrom cryptography.x509 import (\n    Certificate,\n    load_der_x509_certificate,\n    load_pem_x509_certificate,\n)\nfrom pydantic import BaseModel\nfrom sigstore_protobuf_specs.dev.sigstore.bundle.v1 import (\n    Bundle,\n    VerificationMaterial,\n)\nfrom sigstore_protobuf_specs.dev.sigstore.common.v1 import (\n    HashAlgorithm,\n    HashOutput,\n    LogId,\n    MessageSignature,\n    PublicKeyIdentifier,\n    X509Certificate,\n    X509CertificateChain,\n)\nfrom sigstore_protobuf_specs.dev.sigstore.rekor.v1 import (\n    Checkpoint,\n    InclusionPromise,\n    InclusionProof,\n    KindVersion,\n    TransparencyLogEntry,\n)\n\nfrom sigstore._internal.rekor import RekorClient\nfrom sigstore._utils import (\n    B64Str,\n    PEMCert,\n    base64_encode_pem_cert,\n    cert_is_leaf,\n    cert_is_root_ca,\n    sha256_streaming,\n)\nfrom sigstore.errors import Error\nfrom sigstore.transparency import LogEntry, LogInclusionProof\n\nlogger = logging.getLogger(__name__)\n\n_BUNDLE_0_1 = \"application/vnd.dev.sigstore.bundle+json;version=0.1\"\n_BUNDLE_0_2 = \"application/vnd.dev.sigstore.bundle+json;version=0.2\"\n_KNOWN_BUNDLE_TYPES = {\n    _BUNDLE_0_1,\n    _BUNDLE_0_2,\n}\n\n\nclass VerificationResult(BaseModel):\n    \n    Represents the result of a verification operation.\n\n    Results are boolish, and failures contain a reason (and potentially\n    some additional context).\n    \n\n    success: bool\n    \n    Represents the status of this result.\n    \n\n    def __bool__(self) -> bool:\n        \n        Returns a boolean representation of this result.\n\n        `VerificationSuccess` is always `True`, and `VerificationFailure`\n        is always `False`.\n        \n        return self.success\n\n\nclass VerificationSuccess(VerificationResult):\n    \n    The verification completed successfully,\n    \n\n    success: bool = True\n    \n    See `VerificationResult.success`.\n    \n\n\nclass VerificationFailure(VerificationResult):\n    \n    The verification failed, due to `reason`.\n    \n\n    success: bool = False\n    \n    See `VerificationResult.success`.\n    \n\n    reason: str\n    \n    A human-readable explanation or description of the verification failure.\n    \n\n\nclass InvalidMaterials(Error):\n    \n    Raised when the associated `VerificationMaterials` are invalid in some way.\n    \n\n    def diagnostics(self) -> str:\n        Returns diagnostics for the error.\n\n        return dedent(\n            f\\\n        An issue occurred while parsing the verification materials.\n\n        The provided verification materials are malformed and may have been\n        modified maliciously.\n\n        Additional context:\n\n        {self}\n        \n        )\n\n\nclass RekorEntryMissing(Exception):\n    \n    Raised if `VerificationMaterials.rekor_entry()` fails to find an entry\n    in the Rekor log.\n\n    This is an internal exception; users should not see it.\n    \n\n    pass\n\n\nclass InvalidRekorEntry(InvalidMaterials):\n    \n    Raised if the effective Rekor entry in `VerificationMaterials.rekor_entry()`\n    does not match the other materials in `VerificationMaterials`.\n\n    This can only happen in two scenarios:\n\n    * A user has supplied the wrong offline entry, potentially maliciously;\n    * The Rekor log responded with the wrong entry, suggesting a server error.\n    \n\n    pass\n\n\n@dataclass(init=False)\nclass VerificationMaterials:\n    \n    Represents the materials needed to perform a Sigstore verification.\n    \n\n    input_digest: bytes\n    \n    The SHA256 hash of the verification input, as raw bytes.\n    \n\n    certificate: Certificate\n    \n    The certificate that attests to and contains the public signing key.\n    \n\n    signature: bytes\n    \n    The raw signature.\n    \n\n    _offline: bool\n    \n    Whether to do offline Rekor entry verification.\n\n    NOTE: This is intentionally not a public field, since it's slightly\n    mismatched against the other members of `VerificationMaterials` -- it's\n    more of an option than a piece of verification material.\n    \n\n    _rekor_entry: LogEntry | None\n    \n    An optional Rekor entry.\n\n    If a Rekor entry is supplied **and** `offline` is set to `True`,\n    verification will be done against this entry rather than the against the\n    online transparency log. If not provided **or** `offline` is `False` (the\n    default), then the online transparency log will be used.\n\n    NOTE: This is **intentionally not a public field**. The `rekor_entry()`\n    method should be used to access a Rekor log entry for these materials,\n    as it performs the online lookup if an offline entry is not provided\n    and, **critically**, validates that the entry's contents match the other\n    signing materials. Without this check an adversary could present a\n    **valid but unrelated** Rekor entry during verification, similar\n    to CVE-2022-36056 in cosign.\n\n    TODO: Support multiple entries here, with verification contingent on\n    all being valid.\n    \n\n    def __init__(\n        self,\n        *,\n        input_: IO[bytes],\n        cert_pem: PEMCert,\n        signature: bytes,\n        offline: bool = False,\n        rekor_entry: LogEntry | None,\n    ):\n        \n        Create a new `VerificationMaterials` from the given materials.\n\n        `offline` controls the behavior of any subsequent verification over\n        these materials: if `True`, the supplied Rekor entry (which must\n        be supplied) will be verified via its Signed Entry Timestamp, but\n        its proof of inclusion will not be checked. This is a slightly weaker\n        verification mode, as it demonstrates that an entry has been signed by\n        the log but not necessarily included in it.\n\n        Effect: `input_` is consumed as part of construction.\n        \n\n        self.input_digest = sha256_streaming(input_)\n        self.certificate = load_pem_x509_certificate(cert_pem.encode())\n        self.signature = signature\n\n        # Invariant: requesting offline verification means that a Rekor entry\n        # *must* be provided.\n        if offline and not rekor_entry:\n            raise InvalidMaterials(\"offline verification requires a Rekor entry\")\n\n        self._offline = offline\n        self._rekor_entry = rekor_entry\n\n    @classmethod\n    def from_bundle(\n        cls, *, input_: IO[bytes], bundle: Bundle, offline: bool = False\n    ) -> VerificationMaterials:\n        \n        Create a new `VerificationMaterials` from the given Sigstore bundle.\n\n        Effect: `input_` is consumed as part of construction.\n        \n        if bundle.media_type not in _KNOWN_BUNDLE_TYPES:\n            raise InvalidMaterials(f\"unsupported bundle format: {bundle.media_type}\")\n\n        certs = bundle.verification_material.x509_certificate_chain.certificates\n\n        if len(certs) == 0:\n            raise InvalidMaterials(\"expected non-empty certificate chain in bundle\")\n\n        # Per client policy in protobuf-specs: the first entry in the chain\n        # MUST be a leaf certificate, and the rest of the chain MUST NOT\n        # include a root CA or any intermediate CAs that appear in an\n        # independent root of trust.\n        #\n        # We expect some old bundles to violate the rules around root\n        # and intermediate CAs, so we issue warnings and not hard errors\n        # in those cases.\n        leaf_cert, *chain_certs = [\n            load_der_x509_certificate(cert.raw_bytes) for cert in certs\n        ]\n        if not cert_is_leaf(leaf_cert):\n            raise InvalidMaterials(\n                \"bundle contains an invalid leaf or non-leaf certificate in the leaf position\"\n            )\n\n        for chain_cert in chain_certs:\n            # TODO: We should also retrieve the root of trust here and\n            # cross-check against it.\n            if cert_is_root_ca(chain_cert):\n                logger.warning(\n                    \"this bundle contains a root CA, making it subject to misuse\"\n                )\n\n        signature = bundle.message_signature.signature\n\n        tlog_entries = bundle.verification_material.tlog_entries\n        if len(tlog_entries) != 1:\n            raise InvalidMaterials(\n                f\"expected exactly one log entry, got {len(tlog_entries)}\"\n            )\n        tlog_entry = tlog_entries[0]\n\n        # Handling of inclusion promises and proofs varies between bundle\n        # format versions:\n        #\n        # * For 0.1, an inclusion promise is required; the client\n        #   MUST verify the inclusion promise.\n        #   The inclusion proof is NOT required. If provided, it might NOT\n        #   contain a checkpoint; in this case, we ignore it (since it's\n        #   useless without one).\n        #\n        # * For 0.2, an inclusion proof is required; the client MUST\n        #   verify the inclusion proof. The inclusion prof MUST contain\n        #   a checkpoint.\n        #   The inclusion promise is NOT required; if present, the client\n        #   SHOULD verify it.\n\n        inclusion_promise: InclusionPromise | None = tlog_entry.inclusion_promise\n        inclusion_proof: InclusionProof | None = tlog_entry.inclusion_proof\n        if bundle.media_type == _BUNDLE_0_1:\n            if not inclusion_promise:\n                raise InvalidMaterials(\"bundle must contain an inclusion promise\")\n            if inclusion_proof and not inclusion_proof.checkpoint.envelope:\n                logger.debug(\n                    \"0.1 bundle contains inclusion proof without checkpoint; ignoring\"\n                )\n        elif bundle.media_type == _BUNDLE_0_2:\n            if not inclusion_proof:\n                raise InvalidMaterials(\"bundle must contain an inclusion proof\")\n            if not inclusion_proof.checkpoint.envelope:\n                raise InvalidMaterials(\"expected checkpoint in inclusion proof\")\n\n        parsed_inclusion_proof: InclusionProof | None = None\n        if (\n            inclusion_proof is not None\n            and inclusion_proof.checkpoint.envelope is not None\n        ):\n            parsed_inclusion_proof = LogInclusionProof(\n                checkpoint=inclusion_proof.checkpoint.envelope,\n                hashes=[h.hex() for h in inclusion_proof.hashes],\n                log_index=inclusion_proof.log_index,\n                root_hash=inclusion_proof.root_hash.hex(),\n                tree_size=inclusion_proof.tree_size,\n            )\n\n        entry = LogEntry(\n            uuid=None,\n            body=B64Str(base64.b64encode(tlog_entry.canonicalized_body).decode()),\n            integrated_time=tlog_entry.integrated_time,\n            log_id=tlog_entry.log_id.key_id.hex(),\n            log_index=tlog_entry.log_index,\n            inclusion_proof=parsed_inclusion_proof,\n            inclusion_promise=B64Str(\n                base64.b64encode(\n                    tlog_entry.inclusion_promise.signed_entry_timestamp\n                ).decode()\n            ),\n        )\n\n        return cls(\n            input_=input_,\n            cert_pem=PEMCert(leaf_cert.public_bytes(Encoding.PEM).decode()),\n            signature=signature,\n            offline=offline,\n            rekor_entry=entry,\n        )\n\n    @property\n    def has_rekor_entry(self) -> bool:\n        \n        Returns whether or not these `VerificationMaterials` contain a Rekor\n        entry.\n\n        If false, `VerificationMaterials.rekor_entry()` performs an online lookup.\n        \n        return self._rekor_entry is not None\n\n    def rekor_entry(self, client: RekorClient) -> LogEntry:\n        \n        Returns a `LogEntry` for the current signing materials.\n        \n\n        offline = self._offline\n        has_inclusion_promise = (\n            self.has_rekor_entry and self._rekor_entry.inclusion_promise is not None  # type: ignore\n        )\n        has_inclusion_proof = (\n            self.has_rekor_entry\n            and self._rekor_entry.inclusion_proof is not None  # type: ignore\n            and self._rekor_entry.inclusion_proof.checkpoint  # type: ignore\n        )\n\n        logger.debug(\n            f\"has_inclusion_proof={has_inclusion_proof} \"\n            f\"has_inclusion_promise={has_inclusion_promise}\"\n        )\n\n        # This \"expected\" entry is used both to retrieve the Rekor entry\n        # (if we don't have one) *and* to cross-check whatever response\n        # we receive. See below.\n        expected_entry = sigstore_rekor_types.Hashedrekord(\n            kind=\"hashedrekord\",\n            api_version=\"0.0.1\",\n            spec=sigstore_rekor_types.HashedrekordV001Schema(\n                signature=sigstore_rekor_types.Signature1(\n                    content=base64.b64encode(self.signature).decode(),\n                    public_key=sigstore_rekor_types.PublicKey1(\n                        content=base64_encode_pem_cert(self.certificate)\n                    ),\n                ),\n                data=sigstore_rekor_types.Data(\n                    hash=sigstore_rekor_types.Hash(\n                        algorithm=sigstore_rekor_types.Algorithm.SHA256,\n                        value=self.input_digest.hex(),\n                    ),\n                ),\n            ),\n        )\n\n        entry: LogEntry | None = None\n        if offline:\n            logger.debug(\"offline mode; using offline log entry\")\n            # In offline mode, we require either an inclusion proof or an\n            # inclusion promise. Every `LogEntry` has at least one as a\n            # construction invariant, so no additional check is required here.\n            entry = self._rekor_entry\n        else:\n            # In online mode, we require an inclusion proof. If our supplied log\n            # entry doesn't have one, then we perform a lookup.\n            if not has_inclusion_proof:\n                logger.debug(\"retrieving transparency log entry\")\n                entry = client.log.entries.retrieve.post(expected_entry)\n            else:\n                entry = self._rekor_entry\n\n        # No matter what we do above, we must end up with a Rekor entry.\n        if entry is None:\n            raise RekorEntryMissing\n\n        logger.debug(\"Rekor entry: ensuring contents match signing materials\")\n\n        # To catch a potentially dishonest or compromised Rekor instance, we compare\n        # the expected entry (generated above) with the JSON structure returned\n        # by Rekor. If the two don't match, then we have an invalid entry\n        # and can't proceed.\n        actual_body = json.loads(base64.b64decode(entry.body))\n        if actual_body != expected_entry.model_dump(mode=\"json\", by_alias=True):\n            raise InvalidRekorEntry\n\n        return entry\n\n    def to_bundle(self) -> Bundle:\n        Converts VerificationMaterials into a Bundle. Requires that\n        the VerificationMaterials have a Rekor entry loaded. This is\n        the reverse operation of VerificationMaterials.from_bundle()\n        \n        if not self.has_rekor_entry:\n            raise InvalidMaterials(\n                \"Must have Rekor entry before converting to a Bundle\"\n            )\n        rekor_entry: LogEntry = self._rekor_entry  # type: ignore[assignment]\n\n        inclusion_proof: InclusionProof | None = None\n        if rekor_entry.inclusion_proof is not None:\n            inclusion_proof = InclusionProof(\n                log_index=rekor_entry.inclusion_proof.log_index,\n                root_hash=bytes.fromhex(rekor_entry.inclusion_proof.root_hash),\n                tree_size=rekor_entry.inclusion_proof.tree_size,\n                hashes=[\n                    bytes.fromhex(hash_hex)\n                    for hash_hex in rekor_entry.inclusion_proof.hashes\n                ],\n                checkpoint=Checkpoint(envelope=rekor_entry.inclusion_proof.checkpoint),\n            )\n\n        inclusion_promise: InclusionPromise | None = None\n        if rekor_entry.inclusion_promise:\n            inclusion_promise = InclusionPromise(\n                signed_entry_timestamp=base64.b64decode(rekor_entry.inclusion_promise)\n            )\n\n        bundle = Bundle(\n            media_type=\"application/vnd.dev.sigstore.bundle+json;version=0.2\",\n            verification_material=VerificationMaterial(\n                public_key=PublicKeyIdentifier(),\n                x509_certificate_chain=X509CertificateChain(\n                    certificates=[\n                        X509Certificate(\n                            raw_bytes=self.certificate.public_bytes(Encoding.DER)\n                        )\n                    ]\n                ),\n                tlog_entries=[\n                    TransparencyLogEntry(\n                        log_index=rekor_entry.log_index,\n                        log_id=LogId(key_id=bytes.fromhex(rekor_entry.log_id)),\n                        kind_version=KindVersion(kind=\"hashedrekord\", version=\"0.0.1\"),\n                        integrated_time=rekor_entry.integrated_time,\n                        inclusion_promise=inclusion_promise,\n                        inclusion_proof=inclusion_proof,\n                        canonicalized_body=base64.b64decode(rekor_entry.body),\n                    )\n                ],\n            ),\n            message_signature=MessageSignature(\n                message_digest=HashOutput(\n                    algorithm=HashAlgorithm.SHA2_256,\n                    digest=self.input_digest,\n                ),\n                signature=self.signature,\n            ),\n        )\n        return bundle\n"
            }
        ],
        "code_chunks": {
            "imports": [
                "import base64",
                "import json",
                "import logging",
                "import sigstore_rekor_types"
            ],
            "functions": [],
            "classes": [
                "class VerificationResult(BaseModel):\n    \n\n    success: bool\n    \n\n    def __bool__(self) -> bool:\n        \n        return self.success",
                "class VerificationSuccess(VerificationResult):\n    \n\n    success: bool = True\n    ",
                "class VerificationFailure(VerificationResult):\n    \n\n    success: bool = False\n    \n\n    reason: str\n    ",
                "class InvalidMaterials(Error):\n    \n\n    def diagnostics(self) -> str:\n        \n\n        return dedent(\n            f\\\n        An issue occurred while parsing the verification materials.\n\n        The provided verification materials are malformed and may have been\n        modified maliciously.\n\n        Additional context:\n\n        {self}\n        \n        )",
                "class RekorEntryMissing(Exception):\n    \n\n    pass",
                "class InvalidRekorEntry(InvalidMaterials):\n    \n\n    pass"
            ],
            "documentation": [
                "\nCommon (base) models for the verification APIs.\n"
            ],
            "other": [
                "# Copyright 2022 The Sigstore Authors",
                "#",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");",
                "# you may not use this file except in compliance with the License.",
                "# You may obtain a copy of the License at",
                "#",
                "#      http://www.apache.org/licenses/LICENSE-2.0",
                "#",
                "# Unless required by applicable law or agreed to in writing, software",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "# See the License for the specific language governing permissions and",
                "# limitations under the License.",
                "from __future__ import annotations",
                "from dataclasses import dataclass",
                "from textwrap import dedent",
                "from typing import IO",
                "from cryptography.hazmat.primitives.serialization import Encoding",
                "from cryptography.x509 import (\n    Certificate,\n    load_der_x509_certificate,\n    load_pem_x509_certificate,\n)",
                "from pydantic import BaseModel",
                "from sigstore_protobuf_specs.dev.sigstore.bundle.v1 import (\n    Bundle,\n    VerificationMaterial,\n)",
                "from sigstore_protobuf_specs.dev.sigstore.common.v1 import (\n    HashAlgorithm,\n    HashOutput,\n    LogId,\n    MessageSignature,\n    PublicKeyIdentifier,\n    X509Certificate,\n    X509CertificateChain,\n)",
                "from sigstore_protobuf_specs.dev.sigstore.rekor.v1 import (\n    Checkpoint,\n    InclusionPromise,\n    InclusionProof,\n    KindVersion,\n    TransparencyLogEntry,\n)",
                "from sigstore._internal.rekor import RekorClient",
                "from sigstore._utils import (\n    B64Str,\n    PEMCert,\n    base64_encode_pem_cert,\n    cert_is_leaf,\n    cert_is_root_ca,\n    sha256_streaming,\n)",
                "from sigstore.errors import Error",
                "from sigstore.transparency import LogEntry, LogInclusionProof",
                "logger = logging.getLogger(__name__)",
                "_BUNDLE_0_1 = \"application/vnd.dev.sigstore.bundle+json;version=0.1\"",
                "_BUNDLE_0_2 = \"application/vnd.dev.sigstore.bundle+json;version=0.2\"",
                "_KNOWN_BUNDLE_TYPES = {\n    _BUNDLE_0_1,\n    _BUNDLE_0_2,\n}",
                "@dataclass(init=False)\nclass VerificationMaterials:\n    \n    Represents the materials needed to perform a Sigstore verification.\n    \n\n    input_digest: bytes\n    \n    The SHA256 hash of the verification input, as raw bytes.\n    \n\n    certificate: Certificate\n    \n    The certificate that attests to and contains the public signing key.\n    \n\n    signature: bytes\n    \n    The raw signature.\n    \n\n    _offline: bool\n    \n    Whether to do offline Rekor entry verification.\n\n    NOTE: This is intentionally not a public field, since it's slightly\n    mismatched against the other members of `VerificationMaterials` -- it's\n    more of an option than a piece of verification material.\n    \n\n    _rekor_entry: LogEntry | None\n    \n    An optional Rekor entry.\n\n    If a Rekor entry is supplied **and** `offline` is set to `True`,\n    verification will be done against this entry rather than the against the\n    online transparency log. If not provided **or** `offline` is `False` (the\n    default), then the online transparency log will be used.\n\n    NOTE: This is **intentionally not a public field**. The `rekor_entry()`\n    method should be used to access a Rekor log entry for these materials,\n    as it performs the online lookup if an offline entry is not provided\n    and, **critically**, validates that the entry's contents match the other\n    signing materials. Without this check an adversary could present a\n    **valid but unrelated** Rekor entry during verification, similar\n    to CVE-2022-36056 in cosign.\n\n    TODO: Support multiple entries here, with verification contingent on\n    all being valid.\n    \n\n    def __init__(\n        self,\n        *,\n        input_: IO[bytes],\n        cert_pem: PEMCert,\n        signature: bytes,\n        offline: bool = False,\n        rekor_entry: LogEntry | None,\n    ):\n        \n        Create a new `VerificationMaterials` from the given materials.\n\n        `offline` controls the behavior of any subsequent verification over\n        these materials: if `True`, the supplied Rekor entry (which must\n        be supplied) will be verified via its Signed Entry Timestamp, but\n        its proof of inclusion will not be checked. This is a slightly weaker\n        verification mode, as it demonstrates that an entry has been signed by\n        the log but not necessarily included in it.\n\n        Effect: `input_` is consumed as part of construction.\n        \n\n        self.input_digest = sha256_streaming(input_)\n        self.certificate = load_pem_x509_certificate(cert_pem.encode())\n        self.signature = signature\n\n        # Invariant: requesting offline verification means that a Rekor entry\n        # *must* be provided.\n        if offline and not rekor_entry:\n            raise InvalidMaterials(\"offline verification requires a Rekor entry\")\n\n        self._offline = offline\n        self._rekor_entry = rekor_entry\n\n    @classmethod\n    def from_bundle(\n        cls, *, input_: IO[bytes], bundle: Bundle, offline: bool = False\n    ) -> VerificationMaterials:\n        \n        Create a new `VerificationMaterials` from the given Sigstore bundle.\n\n        Effect: `input_` is consumed as part of construction.\n        \n        if bundle.media_type not in _KNOWN_BUNDLE_TYPES:\n            raise InvalidMaterials(f\"unsupported bundle format: {bundle.media_type}\")\n\n        certs = bundle.verification_material.x509_certificate_chain.certificates\n\n        if len(certs) == 0:\n            raise InvalidMaterials(\"expected non-empty certificate chain in bundle\")\n\n        # Per client policy in protobuf-specs: the first entry in the chain\n        # MUST be a leaf certificate, and the rest of the chain MUST NOT\n        # include a root CA or any intermediate CAs that appear in an\n        # independent root of trust.\n        #\n        # We expect some old bundles to violate the rules around root\n        # and intermediate CAs, so we issue warnings and not hard errors\n        # in those cases.\n        leaf_cert, *chain_certs = [\n            load_der_x509_certificate(cert.raw_bytes) for cert in certs\n        ]\n        if not cert_is_leaf(leaf_cert):\n            raise InvalidMaterials(\n                \"bundle contains an invalid leaf or non-leaf certificate in the leaf position\"\n            )\n\n        for chain_cert in chain_certs:\n            # TODO: We should also retrieve the root of trust here and\n            # cross-check against it.\n            if cert_is_root_ca(chain_cert):\n                logger.warning(\n                    \"this bundle contains a root CA, making it subject to misuse\"\n                )\n\n        signature = bundle.message_signature.signature\n\n        tlog_entries = bundle.verification_material.tlog_entries\n        if len(tlog_entries) != 1:\n            raise InvalidMaterials(\n                f\"expected exactly one log entry, got {len(tlog_entries)}\"\n            )\n        tlog_entry = tlog_entries[0]\n\n        # Handling of inclusion promises and proofs varies between bundle\n        # format versions:\n        #\n        # * For 0.1, an inclusion promise is required; the client\n        #   MUST verify the inclusion promise.\n        #   The inclusion proof is NOT required. If provided, it might NOT\n        #   contain a checkpoint; in this case, we ignore it (since it's\n        #   useless without one).\n        #\n        # * For 0.2, an inclusion proof is required; the client MUST\n        #   verify the inclusion proof. The inclusion prof MUST contain\n        #   a checkpoint.\n        #   The inclusion promise is NOT required; if present, the client\n        #   SHOULD verify it.\n\n        inclusion_promise: InclusionPromise | None = tlog_entry.inclusion_promise\n        inclusion_proof: InclusionProof | None = tlog_entry.inclusion_proof\n        if bundle.media_type == _BUNDLE_0_1:\n            if not inclusion_promise:\n                raise InvalidMaterials(\"bundle must contain an inclusion promise\")\n            if inclusion_proof and not inclusion_proof.checkpoint.envelope:\n                logger.debug(\n                    \"0.1 bundle contains inclusion proof without checkpoint; ignoring\"\n                )\n        elif bundle.media_type == _BUNDLE_0_2:\n            if not inclusion_proof:\n                raise InvalidMaterials(\"bundle must contain an inclusion proof\")\n            if not inclusion_proof.checkpoint.envelope:\n                raise InvalidMaterials(\"expected checkpoint in inclusion proof\")\n\n        parsed_inclusion_proof: InclusionProof | None = None\n        if (\n            inclusion_proof is not None\n            and inclusion_proof.checkpoint.envelope is not None\n        ):\n            parsed_inclusion_proof = LogInclusionProof(\n                checkpoint=inclusion_proof.checkpoint.envelope,\n                hashes=[h.hex() for h in inclusion_proof.hashes],\n                log_index=inclusion_proof.log_index,\n                root_hash=inclusion_proof.root_hash.hex(),\n                tree_size=inclusion_proof.tree_size,\n            )\n\n        entry = LogEntry(\n            uuid=None,\n            body=B64Str(base64.b64encode(tlog_entry.canonicalized_body).decode()),\n            integrated_time=tlog_entry.integrated_time,\n            log_id=tlog_entry.log_id.key_id.hex(),\n            log_index=tlog_entry.log_index,\n            inclusion_proof=parsed_inclusion_proof,\n            inclusion_promise=B64Str(\n                base64.b64encode(\n                    tlog_entry.inclusion_promise.signed_entry_timestamp\n                ).decode()\n            ),\n        )\n\n        return cls(\n            input_=input_,\n            cert_pem=PEMCert(leaf_cert.public_bytes(Encoding.PEM).decode()),\n            signature=signature,\n            offline=offline,\n            rekor_entry=entry,\n        )\n\n    @property\n    def has_rekor_entry(self) -> bool:\n        \n        Returns whether or not these `VerificationMaterials` contain a Rekor\n        entry.\n\n        If false, `VerificationMaterials.rekor_entry()` performs an online lookup.\n        \n        return self._rekor_entry is not None\n\n    def rekor_entry(self, client: RekorClient) -> LogEntry:\n        \n        Returns a `LogEntry` for the current signing materials.\n        \n\n        offline = self._offline\n        has_inclusion_promise = (\n            self.has_rekor_entry and self._rekor_entry.inclusion_promise is not None  # type: ignore\n        )\n        has_inclusion_proof = (\n            self.has_rekor_entry\n            and self._rekor_entry.inclusion_proof is not None  # type: ignore\n            and self._rekor_entry.inclusion_proof.checkpoint  # type: ignore\n        )\n\n        logger.debug(\n            f\"has_inclusion_proof={has_inclusion_proof} \"\n            f\"has_inclusion_promise={has_inclusion_promise}\"\n        )\n\n        # This \"expected\" entry is used both to retrieve the Rekor entry\n        # (if we don't have one) *and* to cross-check whatever response\n        # we receive. See below.\n        expected_entry = sigstore_rekor_types.Hashedrekord(\n            kind=\"hashedrekord\",\n            api_version=\"0.0.1\",\n            spec=sigstore_rekor_types.HashedrekordV001Schema(\n                signature=sigstore_rekor_types.Signature1(\n                    content=base64.b64encode(self.signature).decode(),\n                    public_key=sigstore_rekor_types.PublicKey1(\n                        content=base64_encode_pem_cert(self.certificate)\n                    ),\n                ),\n                data=sigstore_rekor_types.Data(\n                    hash=sigstore_rekor_types.Hash(\n                        algorithm=sigstore_rekor_types.Algorithm.SHA256,\n                        value=self.input_digest.hex(),\n                    ),\n                ),\n            ),\n        )\n\n        entry: LogEntry | None = None\n        if offline:\n            logger.debug(\"offline mode; using offline log entry\")\n            # In offline mode, we require either an inclusion proof or an\n            # inclusion promise. Every `LogEntry` has at least one as a\n            # construction invariant, so no additional check is required here.\n            entry = self._rekor_entry\n        else:\n            # In online mode, we require an inclusion proof. If our supplied log\n            # entry doesn't have one, then we perform a lookup.\n            if not has_inclusion_proof:\n                logger.debug(\"retrieving transparency log entry\")\n                entry = client.log.entries.retrieve.post(expected_entry)\n            else:\n                entry = self._rekor_entry\n\n        # No matter what we do above, we must end up with a Rekor entry.\n        if entry is None:\n            raise RekorEntryMissing\n\n        logger.debug(\"Rekor entry: ensuring contents match signing materials\")\n\n        # To catch a potentially dishonest or compromised Rekor instance, we compare\n        # the expected entry (generated above) with the JSON structure returned\n        # by Rekor. If the two don't match, then we have an invalid entry\n        # and can't proceed.\n        actual_body = json.loads(base64.b64decode(entry.body))\n        if actual_body != expected_entry.model_dump(mode=\"json\", by_alias=True):\n            raise InvalidRekorEntry\n\n        return entry\n\n    def to_bundle(self) -> Bundle:\n        Converts VerificationMaterials into a Bundle. Requires that\n        the VerificationMaterials have a Rekor entry loaded. This is\n        the reverse operation of VerificationMaterials.from_bundle()\n        \n        if not self.has_rekor_entry:\n            raise InvalidMaterials(\n                \"Must have Rekor entry before converting to a Bundle\"\n            )\n        rekor_entry: LogEntry = self._rekor_entry  # type: ignore[assignment]\n\n        inclusion_proof: InclusionProof | None = None\n        if rekor_entry.inclusion_proof is not None:\n            inclusion_proof = InclusionProof(\n                log_index=rekor_entry.inclusion_proof.log_index,\n                root_hash=bytes.fromhex(rekor_entry.inclusion_proof.root_hash),\n                tree_size=rekor_entry.inclusion_proof.tree_size,\n                hashes=[\n                    bytes.fromhex(hash_hex)\n                    for hash_hex in rekor_entry.inclusion_proof.hashes\n                ],\n                checkpoint=Checkpoint(envelope=rekor_entry.inclusion_proof.checkpoint),\n            )\n\n        inclusion_promise: InclusionPromise | None = None\n        if rekor_entry.inclusion_promise:\n            inclusion_promise = InclusionPromise(\n                signed_entry_timestamp=base64.b64decode(rekor_entry.inclusion_promise)\n            )\n\n        bundle = Bundle(\n            media_type=\"application/vnd.dev.sigstore.bundle+json;version=0.2\",\n            verification_material=VerificationMaterial(\n                public_key=PublicKeyIdentifier(),\n                x509_certificate_chain=X509CertificateChain(\n                    certificates=[\n                        X509Certificate(\n                            raw_bytes=self.certificate.public_bytes(Encoding.DER)\n                        )\n                    ]\n                ),\n                tlog_entries=[\n                    TransparencyLogEntry(\n                        log_index=rekor_entry.log_index,\n                        log_id=LogId(key_id=bytes.fromhex(rekor_entry.log_id)),\n                        kind_version=KindVersion(kind=\"hashedrekord\", version=\"0.0.1\"),\n                        integrated_time=rekor_entry.integrated_time,\n                        inclusion_promise=inclusion_promise,\n                        inclusion_proof=inclusion_proof,\n                        canonicalized_body=base64.b64decode(rekor_entry.body),\n                    )\n                ],\n            ),\n            message_signature=MessageSignature(\n                message_digest=HashOutput(\n                    algorithm=HashAlgorithm.SHA2_256,\n                    digest=self.input_digest,\n                ),\n                signature=self.signature,\n            ),\n        )\n        return bundle"
            ],
            "functions_code": [],
            "functions_docstrings": [],
            "classes_code": [
                "class VerificationResult(BaseModel):\n    \n\n    success: bool\n    \n\n    def __bool__(self) -> bool:\n        \n        return self.success",
                "class VerificationSuccess(VerificationResult):\n    \n\n    success: bool = True\n    ",
                "class VerificationFailure(VerificationResult):\n    \n\n    success: bool = False\n    \n\n    reason: str\n    ",
                "class InvalidMaterials(Error):\n    \n\n    def diagnostics(self) -> str:\n        \n\n        return dedent(\n            f\\\n        An issue occurred while parsing the verification materials.\n\n        The provided verification materials are malformed and may have been\n        modified maliciously.\n\n        Additional context:\n\n        {self}\n        \n        )",
                "class RekorEntryMissing(Exception):\n    \n\n    pass",
                "class InvalidRekorEntry(InvalidMaterials):\n    \n\n    pass"
            ],
            "classes_docstrings": [
                "\n    Represents the result of a verification operation.\n\n    Results are boolish, and failures contain a reason (and potentially\n    some additional context).\n    ",
                "\n    Represents the status of this result.\n    ",
                "\n        Returns a boolean representation of this result.\n\n        `VerificationSuccess` is always `True`, and `VerificationFailure`\n        is always `False`.\n        ",
                "\n    The verification completed successfully,\n    ",
                "\n    See `VerificationResult.success`.\n    ",
                "\n    The verification failed, due to `reason`.\n    ",
                "\n    See `VerificationResult.success`.\n    ",
                "\n    A human-readable explanation or description of the verification failure.\n    ",
                "\n    Raised when the associated `VerificationMaterials` are invalid in some way.\n    ",
                "Returns diagnostics for the error.",
                "\n    Raised if `VerificationMaterials.rekor_entry()` fails to find an entry\n    in the Rekor log.\n\n    This is an internal exception; users should not see it.\n    ",
                "\n    Raised if the effective Rekor entry in `VerificationMaterials.rekor_entry()`\n    does not match the other materials in `VerificationMaterials`.\n\n    This can only happen in two scenarios:\n\n    * A user has supplied the wrong offline entry, potentially maliciously;\n    * The Rekor log responded with the wrong entry, suggesting a server error.\n    "
            ]
        }
    },
    "verify_policy": {
        "markdown": "",
        "code": [
            {
                "verify/policy.py": "# Copyright 2022 The Sigstore Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nAPIs for describing identity verification \"policies\", which describe how the identities\npassed into an individual verification step are verified.\n\n\nfrom __future__ import annotations\n\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import cast\n\ntry:\n    from typing import Protocol\nexcept ImportError:  # pragma: no cover\n    # TODO(ww): Remove when our minimum Python is 3.8.\n    from typing_extensions import Protocol  # type: ignore[assignment]\n\nfrom cryptography.x509 import (\n    Certificate,\n    ExtensionNotFound,\n    ObjectIdentifier,\n    OtherName,\n    RFC822Name,\n    SubjectAlternativeName,\n    UniformResourceIdentifier,\n)\n\nfrom sigstore.verify.models import (\n    VerificationFailure,\n    VerificationResult,\n    VerificationSuccess,\n)\n\nlogger = logging.getLogger(__name__)\n\n# From: https://github.com/sigstore/fulcio/blob/main/docs/oid-info.md\n_OIDC_ISSUER_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.1\")\n_OIDC_GITHUB_WORKFLOW_TRIGGER_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.2\")\n_OIDC_GITHUB_WORKFLOW_SHA_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.3\")\n_OIDC_GITHUB_WORKFLOW_NAME_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.4\")\n_OIDC_GITHUB_WORKFLOW_REPOSITORY_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.5\")\n_OIDC_GITHUB_WORKFLOW_REF_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.6\")\n_OTHERNAME_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.7\")\n\n\nclass _SingleX509ExtPolicy(ABC):\n    \n    An ABC for verification policies that boil down to checking a single\n    X.509 extension's value.\n    \n\n    oid: ObjectIdentifier\n    \n    The OID of the extension being checked.\n    \n\n    def __init__(self, value: str) -> None:\n        \n        Creates the new policy, with `value` as the expected value during\n        verification.\n        \n        self._value = value\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        Verify this policy against `cert`.\n        \n        try:\n            ext = cert.extensions.get_extension_for_oid(self.oid).value\n        except ExtensionNotFound:\n            return VerificationFailure(\n                reason=(\n                    f\"Certificate does not contain {self.__class__.__name__} \"\n                    f\"({self.oid.dotted_string}) extension\"\n                )\n            )\n\n        # NOTE(ww): mypy is confused by the `Extension[ExtensionType]` returned\n        # by `get_extension_for_oid` above.\n        ext_value = ext.value.decode()  # type: ignore[attr-defined]\n        if ext_value != self._value:\n            return VerificationFailure(\n                reason=(\n                    f\"Certificate's {self.__class__.__name__} does not match \"\n                    f\"(got {ext_value}, expected {self._value})\"\n                )\n            )\n\n        return VerificationSuccess()\n\n\nclass OIDCIssuer(_SingleX509ExtPolicy):\n    \n    Verifies the certificate's OIDC issuer, identified by\n    an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.1`.\n    \n\n    oid = _OIDC_ISSUER_OID\n\n\nclass GitHubWorkflowTrigger(_SingleX509ExtPolicy):\n    \n    Verifies the certificate's GitHub Actions workflow trigger,\n    identified by an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.2`.\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_TRIGGER_OID\n\n\nclass GitHubWorkflowSHA(_SingleX509ExtPolicy):\n    \n    Verifies the certificate's GitHub Actions workflow commit SHA,\n    identified by an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.3`.\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_SHA_OID\n\n\nclass GitHubWorkflowName(_SingleX509ExtPolicy):\n    \n    Verifies the certificate's GitHub Actions workflow name,\n    identified by an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.4`.\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_NAME_OID\n\n\nclass GitHubWorkflowRepository(_SingleX509ExtPolicy):\n    \n    Verifies the certificate's GitHub Actions workflow repository,\n    identified by an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.5`.\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_REPOSITORY_OID\n\n\nclass GitHubWorkflowRef(_SingleX509ExtPolicy):\n    \n    Verifies the certificate's GitHub Actions workflow ref,\n    identified by an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.6`.\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_REF_OID\n\n\nclass VerificationPolicy(Protocol):\n    \n    A protocol type describing the interface that all verification policies\n    conform to.\n    \n\n    @abstractmethod\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        Verify the given `cert` against this policy, returning a `VerificationResult`.\n        \n        raise NotImplementedError  # pragma: no cover\n\n\nclass AnyOf:\n    \n    The \"any of\" policy, corresponding to a logical OR between child policies.\n\n    An empty list of child policies is considered trivially invalid.\n    \n\n    def __init__(self, children: list[VerificationPolicy]):\n        \n        Create a new `AnyOf`, with the given child policies.\n        \n        self._children = children\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        Verify `cert` against the policy.\n        \n        verified = any(child.verify(cert) for child in self._children)\n        if verified:\n            return VerificationSuccess()\n        else:\n            return VerificationFailure(\n                reason=f\"0 of {len(self._children)} policies succeeded\"\n            )\n\n\nclass AllOf:\n    \n    The \"all of\" policy, corresponding to a logical AND between child\n    policies.\n\n    An empty list of child policies is considered trivially invalid.\n    \n\n    def __init__(self, children: list[VerificationPolicy]):\n        \n        Create a new `AllOf`, with the given child policies.\n        \n\n        self._children = children\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        Verify `cert` against the policy.\n        \n\n        # Without this, we'd consider empty lists of child policies trivially valid.\n        # This is almost certainly not what the user wants and is a potential\n        # source of API misuse, so we explicitly disallow it.\n        if len(self._children) < 1:\n            return VerificationFailure(reason=\"no child policies to verify\")\n\n        # NOTE(ww): We need the cast here because MyPy can't tell that\n        # `VerificationResult.__bool__` is invariant with\n        # `VerificationSuccess | VerificationFailure`.\n        results = [child.verify(cert) for child in self._children]\n        failures = [\n            cast(VerificationFailure, result).reason for result in results if not result\n        ]\n        if len(failures) > 0:\n            inner_reasons = \", \".join(failures)\n            return VerificationFailure(\n                reason=f\"{len(failures)} of {len(self._children)} policies failed: {inner_reasons}\"\n            )\n        return VerificationSuccess()\n\n\nclass UnsafeNoOp:\n    \n    The \"no-op\" policy, corresponding to a no-op \"verification\".\n\n    **This policy is fundamentally insecure. You cannot use it safely.\n    It must not be used to verify any sort of certificate identity, because\n    it cannot do so. Using this policy is equivalent to reducing the\n    verification proof down to an integrity check against a completely\n    untrusted and potentially attacker-created signature. It must only\n    be used for testing purposes.**\n    \n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        Verify `cert` against the policy.\n        \n\n        logger.warning(\n            \"unsafe (no-op) verification policy used! no verification performed!\"\n        )\n        return VerificationSuccess()\n\n\nclass Identity:\n    \n    Verifies the certificate's \"identity\", corresponding to the X.509v3 SAN.\n    Identities are verified modulo an OIDC issuer, so the issuer's URI\n    is also required.\n\n    Supported SAN types include emails, URIs, and Sigstore-specific \"other names\".\n    \n\n    def __init__(self, *, identity: str, issuer: str):\n        \n        Create a new `Identity`, with the given expected identity and issuer values.\n        \n\n        self._identity = identity\n        self._issuer = OIDCIssuer(issuer)\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        Verify `cert` against the policy.\n        \n\n        issuer_verified: VerificationResult = self._issuer.verify(cert)\n        if not issuer_verified:\n            return issuer_verified\n\n        # Build a set of all valid identities.\n        san_ext = cert.extensions.get_extension_for_class(SubjectAlternativeName).value\n        all_sans = set(san_ext.get_values_for_type(RFC822Name))\n        all_sans.update(san_ext.get_values_for_type(UniformResourceIdentifier))\n        all_sans.update(\n            [\n                on.value.decode()\n                for on in san_ext.get_values_for_type(OtherName)\n                if on.type_id == _OTHERNAME_OID\n            ]\n        )\n\n        verified = self._identity in all_sans\n        if not verified:\n            return VerificationFailure(\n                reason=f\"Certificate's SANs do not match {self._identity}; actual SANs: {all_sans}\"\n            )\n\n        return VerificationSuccess()\n"
            }
        ],
        "code_chunks": {
            "imports": [
                "import logging"
            ],
            "functions": [],
            "classes": [
                "class _SingleX509ExtPolicy(ABC):\n    \n\n    oid: ObjectIdentifier\n    \n\n    def __init__(self, value: str) -> None:\n        \n        self._value = value\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        try:\n            ext = cert.extensions.get_extension_for_oid(self.oid).value\n        except ExtensionNotFound:\n            return VerificationFailure(\n                reason=(\n                    f\"Certificate does not contain {self.__class__.__name__} \"\n                    f\"({self.oid.dotted_string}) extension\"\n                )\n            )\n\n        # NOTE(ww): mypy is confused by the `Extension[ExtensionType]` returned\n        # by `get_extension_for_oid` above.\n        ext_value = ext.value.decode()  # type: ignore[attr-defined]\n        if ext_value != self._value:\n            return VerificationFailure(\n                reason=(\n                    f\"Certificate's {self.__class__.__name__} does not match \"\n                    f\"(got {ext_value}, expected {self._value})\"\n                )\n            )\n\n        return VerificationSuccess()",
                "class OIDCIssuer(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_ISSUER_OID",
                "class GitHubWorkflowTrigger(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_TRIGGER_OID",
                "class GitHubWorkflowSHA(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_SHA_OID",
                "class GitHubWorkflowName(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_NAME_OID",
                "class GitHubWorkflowRepository(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_REPOSITORY_OID",
                "class GitHubWorkflowRef(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_REF_OID",
                "class VerificationPolicy(Protocol):\n    \n\n    @abstractmethod\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        raise NotImplementedError  # pragma: no cover",
                "class AnyOf:\n    \n\n    def __init__(self, children: list[VerificationPolicy]):\n        \n        self._children = children\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        verified = any(child.verify(cert) for child in self._children)\n        if verified:\n            return VerificationSuccess()\n        else:\n            return VerificationFailure(\n                reason=f\"0 of {len(self._children)} policies succeeded\"\n            )",
                "class AllOf:\n    \n\n    def __init__(self, children: list[VerificationPolicy]):\n        \n\n        self._children = children\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n\n        # Without this, we'd consider empty lists of child policies trivially valid.\n        # This is almost certainly not what the user wants and is a potential\n        # source of API misuse, so we explicitly disallow it.\n        if len(self._children) < 1:\n            return VerificationFailure(reason=\"no child policies to verify\")\n\n        # NOTE(ww): We need the cast here because MyPy can't tell that\n        # `VerificationResult.__bool__` is invariant with\n        # `VerificationSuccess | VerificationFailure`.\n        results = [child.verify(cert) for child in self._children]\n        failures = [\n            cast(VerificationFailure, result).reason for result in results if not result\n        ]\n        if len(failures) > 0:\n            inner_reasons = \", \".join(failures)\n            return VerificationFailure(\n                reason=f\"{len(failures)} of {len(self._children)} policies failed: {inner_reasons}\"\n            )\n        return VerificationSuccess()",
                "class UnsafeNoOp:\n    \n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n\n        logger.warning(\n            \"unsafe (no-op) verification policy used! no verification performed!\"\n        )\n        return VerificationSuccess()",
                "class Identity:\n    \n\n    def __init__(self, *, identity: str, issuer: str):\n        \n\n        self._identity = identity\n        self._issuer = OIDCIssuer(issuer)\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n\n        issuer_verified: VerificationResult = self._issuer.verify(cert)\n        if not issuer_verified:\n            return issuer_verified\n\n        # Build a set of all valid identities.\n        san_ext = cert.extensions.get_extension_for_class(SubjectAlternativeName).value\n        all_sans = set(san_ext.get_values_for_type(RFC822Name))\n        all_sans.update(san_ext.get_values_for_type(UniformResourceIdentifier))\n        all_sans.update(\n            [\n                on.value.decode()\n                for on in san_ext.get_values_for_type(OtherName)\n                if on.type_id == _OTHERNAME_OID\n            ]\n        )\n\n        verified = self._identity in all_sans\n        if not verified:\n            return VerificationFailure(\n                reason=f\"Certificate's SANs do not match {self._identity}; actual SANs: {all_sans}\"\n            )\n\n        return VerificationSuccess()"
            ],
            "documentation": [
                "\nAPIs for describing identity verification \"policies\", which describe how the identities\npassed into an individual verification step are verified.\n"
            ],
            "other": [
                "# Copyright 2022 The Sigstore Authors",
                "#",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");",
                "# you may not use this file except in compliance with the License.",
                "# You may obtain a copy of the License at",
                "#",
                "#      http://www.apache.org/licenses/LICENSE-2.0",
                "#",
                "# Unless required by applicable law or agreed to in writing, software",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "# See the License for the specific language governing permissions and",
                "# limitations under the License.",
                "from __future__ import annotations",
                "from abc import ABC, abstractmethod",
                "from typing import cast",
                "try:\n    from typing import Protocol\nexcept ImportError:  # pragma: no cover\n    # TODO(ww): Remove when our minimum Python is 3.8.\n    from typing_extensions import Protocol  # type: ignore[assignment]",
                "from cryptography.x509 import (\n    Certificate,\n    ExtensionNotFound,\n    ObjectIdentifier,\n    OtherName,\n    RFC822Name,\n    SubjectAlternativeName,\n    UniformResourceIdentifier,\n)",
                "from sigstore.verify.models import (\n    VerificationFailure,\n    VerificationResult,\n    VerificationSuccess,\n)",
                "logger = logging.getLogger(__name__)",
                "# From: https://github.com/sigstore/fulcio/blob/main/docs/oid-info.md",
                "_OIDC_ISSUER_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.1\")",
                "_OIDC_GITHUB_WORKFLOW_TRIGGER_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.2\")",
                "_OIDC_GITHUB_WORKFLOW_SHA_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.3\")",
                "_OIDC_GITHUB_WORKFLOW_NAME_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.4\")",
                "_OIDC_GITHUB_WORKFLOW_REPOSITORY_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.5\")",
                "_OIDC_GITHUB_WORKFLOW_REF_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.6\")",
                "_OTHERNAME_OID = ObjectIdentifier(\"1.3.6.1.4.1.57264.1.7\")"
            ],
            "functions_code": [],
            "functions_docstrings": [],
            "classes_code": [
                "class _SingleX509ExtPolicy(ABC):\n    \n\n    oid: ObjectIdentifier\n    \n\n    def __init__(self, value: str) -> None:\n        \n        self._value = value\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        try:\n            ext = cert.extensions.get_extension_for_oid(self.oid).value\n        except ExtensionNotFound:\n            return VerificationFailure(\n                reason=(\n                    f\"Certificate does not contain {self.__class__.__name__} \"\n                    f\"({self.oid.dotted_string}) extension\"\n                )\n            )\n\n        # NOTE(ww): mypy is confused by the `Extension[ExtensionType]` returned\n        # by `get_extension_for_oid` above.\n        ext_value = ext.value.decode()  # type: ignore[attr-defined]\n        if ext_value != self._value:\n            return VerificationFailure(\n                reason=(\n                    f\"Certificate's {self.__class__.__name__} does not match \"\n                    f\"(got {ext_value}, expected {self._value})\"\n                )\n            )\n\n        return VerificationSuccess()",
                "class OIDCIssuer(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_ISSUER_OID",
                "class GitHubWorkflowTrigger(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_TRIGGER_OID",
                "class GitHubWorkflowSHA(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_SHA_OID",
                "class GitHubWorkflowName(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_NAME_OID",
                "class GitHubWorkflowRepository(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_REPOSITORY_OID",
                "class GitHubWorkflowRef(_SingleX509ExtPolicy):\n    \n\n    oid = _OIDC_GITHUB_WORKFLOW_REF_OID",
                "class VerificationPolicy(Protocol):\n    \n\n    @abstractmethod\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        raise NotImplementedError  # pragma: no cover",
                "class AnyOf:\n    \n\n    def __init__(self, children: list[VerificationPolicy]):\n        \n        self._children = children\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n        verified = any(child.verify(cert) for child in self._children)\n        if verified:\n            return VerificationSuccess()\n        else:\n            return VerificationFailure(\n                reason=f\"0 of {len(self._children)} policies succeeded\"\n            )",
                "class AllOf:\n    \n\n    def __init__(self, children: list[VerificationPolicy]):\n        \n\n        self._children = children\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n\n        # Without this, we'd consider empty lists of child policies trivially valid.\n        # This is almost certainly not what the user wants and is a potential\n        # source of API misuse, so we explicitly disallow it.\n        if len(self._children) < 1:\n            return VerificationFailure(reason=\"no child policies to verify\")\n\n        # NOTE(ww): We need the cast here because MyPy can't tell that\n        # `VerificationResult.__bool__` is invariant with\n        # `VerificationSuccess | VerificationFailure`.\n        results = [child.verify(cert) for child in self._children]\n        failures = [\n            cast(VerificationFailure, result).reason for result in results if not result\n        ]\n        if len(failures) > 0:\n            inner_reasons = \", \".join(failures)\n            return VerificationFailure(\n                reason=f\"{len(failures)} of {len(self._children)} policies failed: {inner_reasons}\"\n            )\n        return VerificationSuccess()",
                "class UnsafeNoOp:\n    \n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n\n        logger.warning(\n            \"unsafe (no-op) verification policy used! no verification performed!\"\n        )\n        return VerificationSuccess()",
                "class Identity:\n    \n\n    def __init__(self, *, identity: str, issuer: str):\n        \n\n        self._identity = identity\n        self._issuer = OIDCIssuer(issuer)\n\n    def verify(self, cert: Certificate) -> VerificationResult:\n        \n\n        issuer_verified: VerificationResult = self._issuer.verify(cert)\n        if not issuer_verified:\n            return issuer_verified\n\n        # Build a set of all valid identities.\n        san_ext = cert.extensions.get_extension_for_class(SubjectAlternativeName).value\n        all_sans = set(san_ext.get_values_for_type(RFC822Name))\n        all_sans.update(san_ext.get_values_for_type(UniformResourceIdentifier))\n        all_sans.update(\n            [\n                on.value.decode()\n                for on in san_ext.get_values_for_type(OtherName)\n                if on.type_id == _OTHERNAME_OID\n            ]\n        )\n\n        verified = self._identity in all_sans\n        if not verified:\n            return VerificationFailure(\n                reason=f\"Certificate's SANs do not match {self._identity}; actual SANs: {all_sans}\"\n            )\n\n        return VerificationSuccess()"
            ],
            "classes_docstrings": [
                "\n    An ABC for verification policies that boil down to checking a single\n    X.509 extension's value.\n    ",
                "\n    The OID of the extension being checked.\n    ",
                "\n        Creates the new policy, with `value` as the expected value during\n        verification.\n        ",
                "\n        Verify this policy against `cert`.\n        ",
                "\n    Verifies the certificate's OIDC issuer, identified by\n    an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.1`.\n    ",
                "\n    Verifies the certificate's GitHub Actions workflow trigger,\n    identified by an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.2`.\n    ",
                "\n    Verifies the certificate's GitHub Actions workflow commit SHA,\n    identified by an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.3`.\n    ",
                "\n    Verifies the certificate's GitHub Actions workflow name,\n    identified by an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.4`.\n    ",
                "\n    Verifies the certificate's GitHub Actions workflow repository,\n    identified by an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.5`.\n    ",
                "\n    Verifies the certificate's GitHub Actions workflow ref,\n    identified by an X.509v3 extension tagged with `1.3.6.1.4.1.57264.1.6`.\n    ",
                "\n    A protocol type describing the interface that all verification policies\n    conform to.\n    ",
                "\n        Verify the given `cert` against this policy, returning a `VerificationResult`.\n        ",
                "\n    The \"any of\" policy, corresponding to a logical OR between child policies.\n\n    An empty list of child policies is considered trivially invalid.\n    ",
                "\n        Create a new `AnyOf`, with the given child policies.\n        ",
                "\n        Verify `cert` against the policy.\n        ",
                "\n    The \"all of\" policy, corresponding to a logical AND between child\n    policies.\n\n    An empty list of child policies is considered trivially invalid.\n    ",
                "\n        Create a new `AllOf`, with the given child policies.\n        ",
                "\n        Verify `cert` against the policy.\n        ",
                "\n    The \"no-op\" policy, corresponding to a no-op \"verification\".\n\n    **This policy is fundamentally insecure. You cannot use it safely.\n    It must not be used to verify any sort of certificate identity, because\n    it cannot do so. Using this policy is equivalent to reducing the\n    verification proof down to an integrity check against a completely\n    untrusted and potentially attacker-created signature. It must only\n    be used for testing purposes.**\n    ",
                "\n        Verify `cert` against the policy.\n        ",
                "\n    Verifies the certificate's \"identity\", corresponding to the X.509v3 SAN.\n    Identities are verified modulo an OIDC issuer, so the issuer's URI\n    is also required.\n\n    Supported SAN types include emails, URIs, and Sigstore-specific \"other names\".\n    ",
                "\n        Create a new `Identity`, with the given expected identity and issuer values.\n        ",
                "\n        Verify `cert` against the policy.\n        "
            ]
        }
    },
    "verify_verifier": {
        "markdown": "",
        "code": [
            {
                "verify/verifier.py": "# Copyright 2022 The Sigstore Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nVerification API machinery.\n\n\nfrom __future__ import annotations\n\nimport base64\nimport datetime\nimport logging\nfrom typing import List, cast\n\nfrom cryptography.exceptions import InvalidSignature\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import ec\nfrom cryptography.hazmat.primitives.asymmetric.utils import Prehashed\nfrom cryptography.x509 import Certificate, ExtendedKeyUsage, KeyUsage\nfrom cryptography.x509.oid import ExtendedKeyUsageOID\nfrom OpenSSL.crypto import (  # type: ignore[import-untyped]\n    X509,\n    X509Store,\n    X509StoreContext,\n    X509StoreContextError,\n)\nfrom pydantic import ConfigDict\n\nfrom sigstore._internal.merkle import (\n    InvalidInclusionProofError,\n    verify_merkle_inclusion,\n)\nfrom sigstore._internal.rekor.checkpoint import (\n    CheckpointError,\n    verify_checkpoint,\n)\nfrom sigstore._internal.rekor.client import RekorClient\nfrom sigstore._internal.set import InvalidSETError, verify_set\nfrom sigstore._internal.tuf import TrustUpdater\nfrom sigstore._utils import B64Str, HexStr\nfrom sigstore.verify.models import InvalidRekorEntry as InvalidRekorEntryError\nfrom sigstore.verify.models import RekorEntryMissing as RekorEntryMissingError\nfrom sigstore.verify.models import (\n    VerificationFailure,\n    VerificationMaterials,\n    VerificationResult,\n    VerificationSuccess,\n)\nfrom sigstore.verify.policy import VerificationPolicy\n\nlogger = logging.getLogger(__name__)\n\n\nclass LogEntryMissing(VerificationFailure):\n    \n    A specialization of `VerificationFailure` for transparency log lookup failures,\n    with additional lookup context.\n    \n\n    reason: (\n        str\n    ) = \"The transparency log has no entry for the given verification materials\"\n\n    signature: B64Str\n    \n    The signature present during lookup failure, encoded with base64.\n    \n\n    artifact_hash: HexStr\n    \n    The artifact hash present during lookup failure, encoded as a hex string.\n    \n\n\nclass CertificateVerificationFailure(VerificationFailure):\n    \n    A specialization of `VerificationFailure` for certificate signature\n    verification failures, with additional exception context.\n    \n\n    # Needed for the `exception` field above, since exceptions are\n    # not trivially serializable.\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    reason: str = \"Failed to verify signing certificate\"\n    exception: Exception\n\n\nclass Verifier:\n    \n    The primary API for verification operations.\n    \n\n    def __init__(\n        self, *, rekor: RekorClient, fulcio_certificate_chain: List[Certificate]\n    ):\n        \n        Create a new `Verifier`.\n\n        `rekor` is a `RekorClient` capable of connecting to a Rekor instance\n        containing logs for the file(s) being verified.\n\n        `fulcio_certificate_chain` is a list of PEM-encoded X.509 certificates,\n        establishing the trust chain for the signing certificate and signature.\n        \n        self._rekor = rekor\n\n        self._fulcio_certificate_chain: List[X509] = []\n        for parent_cert in fulcio_certificate_chain:\n            parent_cert_ossl = X509.from_cryptography(parent_cert)\n            self._fulcio_certificate_chain.append(parent_cert_ossl)\n\n    @classmethod\n    def production(cls) -> Verifier:\n        \n        Return a `Verifier` instance configured against Sigstore's production-level services.\n        \n        updater = TrustUpdater.production()\n        return cls(\n            rekor=RekorClient.production(updater),\n            fulcio_certificate_chain=updater.get_fulcio_certs(),\n        )\n\n    @classmethod\n    def staging(cls) -> Verifier:\n        \n        Return a `Verifier` instance configured against Sigstore's staging-level services.\n        \n        updater = TrustUpdater.staging()\n        return cls(\n            rekor=RekorClient.staging(updater),\n            fulcio_certificate_chain=updater.get_fulcio_certs(),\n        )\n\n    def verify(\n        self,\n        materials: VerificationMaterials,\n        policy: VerificationPolicy,\n    ) -> VerificationResult:\n        Public API for verifying.\n\n        `materials` are the `VerificationMaterials` to verify.\n\n        `policy` is the `VerificationPolicy` to verify against.\n\n        Returns a `VerificationResult` which will be truthy or falsey depending on\n        success.\n        \n\n        # NOTE: The `X509Store` object currently cannot have its time reset once the `set_time`\n        # method been called on it. To get around this, we construct a new one for every `verify`\n        # call.\n        store = X509Store()\n        for parent_cert_ossl in self._fulcio_certificate_chain:\n            store.add_cert(parent_cert_ossl)\n\n        # In order to verify an artifact, we need to achieve the following:\n        #\n        # 1) Verify that the signing certificate is signed by the certificate\n        #    chain and that the signing certificate was valid at the time\n        #    of signing.\n        # 2) Verify that the signing certificate belongs to the signer.\n        # 3) Verify that the artifact signature was signed by the public key in the\n        #    signing certificate.\n        # 4) Verify that the Rekor entry is consistent with the other signing\n        #    materials (preventing CVE-2022-36056)\n        # 5) Verify the inclusion proof supplied by Rekor for this artifact,\n        #    if we're doing online verification.\n        # 6) Verify the Signed Entry Timestamp (SET) supplied by Rekor for this\n        #    artifact.\n        # 7) Verify that the signing certificate was valid at the time of\n        #    signing by comparing the expiry against the integrated timestamp.\n\n        # 1) Verify that the signing certificate is signed by the root certificate and that the\n        #    signing certificate was valid at the time of signing.\n        sign_date = materials.certificate.not_valid_before\n        cert_ossl = X509.from_cryptography(materials.certificate)\n\n        store.set_time(sign_date)\n        store_ctx = X509StoreContext(store, cert_ossl)\n        try:\n            store_ctx.verify_certificate()\n        except X509StoreContextError as store_ctx_error:\n            return CertificateVerificationFailure(\n                exception=store_ctx_error,\n            )\n\n        # 2) Check that the signing certificate contains the proof claim as the subject\n        # Check usage is \"digital signature\"\n        usage_ext = materials.certificate.extensions.get_extension_for_class(KeyUsage)\n        if not usage_ext.value.digital_signature:\n            return VerificationFailure(\n                reason=\"Key usage is not of type `digital signature`\"\n            )\n\n        # Check that extended usage contains \"code signing\"\n        extended_usage_ext = materials.certificate.extensions.get_extension_for_class(\n            ExtendedKeyUsage\n        )\n        if ExtendedKeyUsageOID.CODE_SIGNING not in extended_usage_ext.value:\n            return VerificationFailure(\n                reason=\"Extended usage does not contain `code signing`\"\n            )\n\n        policy_check = policy.verify(materials.certificate)\n        if not policy_check:\n            return policy_check\n\n        logger.debug(\"Successfully verified signing certificate validity...\")\n\n        # 3) Verify that the signature was signed by the public key in the signing certificate\n        try:\n            signing_key = materials.certificate.public_key()\n            signing_key = cast(ec.EllipticCurvePublicKey, signing_key)\n            signing_key.verify(\n                materials.signature,\n                materials.input_digest,\n                ec.ECDSA(Prehashed(hashes.SHA256())),\n            )\n        except InvalidSignature:\n            return VerificationFailure(reason=\"Signature is invalid for input\")\n\n        logger.debug(\"Successfully verified signature...\")\n\n        # 4) Retrieve the Rekor entry for this artifact (potentially from\n        # an offline entry), confirming its consistency with the other\n        # artifacts in the process.\n        try:\n            entry = materials.rekor_entry(self._rekor)\n        except RekorEntryMissingError:\n            return LogEntryMissing(\n                signature=B64Str(base64.b64encode(materials.signature).decode()),\n                artifact_hash=HexStr(materials.input_digest.hex()),\n            )\n        except InvalidRekorEntryError:\n            return VerificationFailure(\n                reason=\"Rekor entry contents do not match other signing materials\"\n            )\n\n        # 5) Verify the inclusion proof supplied by Rekor for this artifact.\n        #\n        # The inclusion proof should always be present in the online case. In\n        # the offline case, if it is present, we verify it.\n        if entry.inclusion_proof and entry.inclusion_proof.checkpoint:\n            try:\n                verify_merkle_inclusion(entry)\n            except InvalidInclusionProofError as exc:\n                return VerificationFailure(\n                    reason=f\"invalid Rekor inclusion proof: {exc}\"\n                )\n\n            try:\n                verify_checkpoint(self._rekor, entry)\n            except CheckpointError as exc:\n                return VerificationFailure(reason=f\"invalid Rekor root hash: {exc}\")\n\n            logger.debug(\n                f\"successfully verified inclusion proof: index={entry.log_index}\"\n            )\n        elif not materials._offline:\n            # Paranoia: if we weren't given an inclusion proof, then\n            # this *must* have been offline verification. If it was online\n            # then we've somehow entered an invalid state, so fail.\n            return VerificationFailure(reason=\"missing Rekor inclusion proof\")\n        else:\n            logger.warning(\n                \"inclusion proof not present in bundle: skipping due to offline verification\"\n            )\n\n        # 6) Verify the Signed Entry Timestamp (SET) supplied by Rekor for this artifact\n        if entry.inclusion_promise:\n            try:\n                verify_set(self._rekor, entry)\n                logger.debug(\n                    f\"successfully verified inclusion promise: index={entry.log_index}\"\n                )\n            except InvalidSETError as inval_set:\n                return VerificationFailure(\n                    reason=f\"invalid Rekor entry SET: {inval_set}\"\n                )\n\n        # 7) Verify that the signing certificate was valid at the time of signing\n        integrated_time = datetime.datetime.utcfromtimestamp(entry.integrated_time)\n        if not (\n            materials.certificate.not_valid_before\n            <= integrated_time\n            <= materials.certificate.not_valid_after\n        ):\n            return VerificationFailure(\n                reason=\"invalid signing cert: expired at time of Rekor entry\"\n            )\n\n        return VerificationSuccess()\n"
            }
        ],
        "code_chunks": {
            "imports": [
                "import base64",
                "import datetime",
                "import logging"
            ],
            "functions": [],
            "classes": [
                "class LogEntryMissing(VerificationFailure):\n    \n\n    reason: (\n        str\n    ) = \"The transparency log has no entry for the given verification materials\"\n\n    signature: B64Str\n    \n\n    artifact_hash: HexStr\n    ",
                "class CertificateVerificationFailure(VerificationFailure):\n    \n\n    # Needed for the `exception` field above, since exceptions are\n    # not trivially serializable.\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    reason: str = \"Failed to verify signing certificate\"\n    exception: Exception",
                "class Verifier:\n    \n\n    def __init__(\n        self, *, rekor: RekorClient, fulcio_certificate_chain: List[Certificate]\n    ):\n        \n        self._rekor = rekor\n\n        self._fulcio_certificate_chain: List[X509] = []\n        for parent_cert in fulcio_certificate_chain:\n            parent_cert_ossl = X509.from_cryptography(parent_cert)\n            self._fulcio_certificate_chain.append(parent_cert_ossl)\n\n    @classmethod\n    def production(cls) -> Verifier:\n        \n        updater = TrustUpdater.production()\n        return cls(\n            rekor=RekorClient.production(updater),\n            fulcio_certificate_chain=updater.get_fulcio_certs(),\n        )\n\n    @classmethod\n    def staging(cls) -> Verifier:\n        \n        updater = TrustUpdater.staging()\n        return cls(\n            rekor=RekorClient.staging(updater),\n            fulcio_certificate_chain=updater.get_fulcio_certs(),\n        )\n\n    def verify(\n        self,\n        materials: VerificationMaterials,\n        policy: VerificationPolicy,\n    ) -> VerificationResult:\n        \n\n        # NOTE: The `X509Store` object currently cannot have its time reset once the `set_time`\n        # method been called on it. To get around this, we construct a new one for every `verify`\n        # call.\n        store = X509Store()\n        for parent_cert_ossl in self._fulcio_certificate_chain:\n            store.add_cert(parent_cert_ossl)\n\n        # In order to verify an artifact, we need to achieve the following:\n        #\n        # 1) Verify that the signing certificate is signed by the certificate\n        #    chain and that the signing certificate was valid at the time\n        #    of signing.\n        # 2) Verify that the signing certificate belongs to the signer.\n        # 3) Verify that the artifact signature was signed by the public key in the\n        #    signing certificate.\n        # 4) Verify that the Rekor entry is consistent with the other signing\n        #    materials (preventing CVE-2022-36056)\n        # 5) Verify the inclusion proof supplied by Rekor for this artifact,\n        #    if we're doing online verification.\n        # 6) Verify the Signed Entry Timestamp (SET) supplied by Rekor for this\n        #    artifact.\n        # 7) Verify that the signing certificate was valid at the time of\n        #    signing by comparing the expiry against the integrated timestamp.\n\n        # 1) Verify that the signing certificate is signed by the root certificate and that the\n        #    signing certificate was valid at the time of signing.\n        sign_date = materials.certificate.not_valid_before\n        cert_ossl = X509.from_cryptography(materials.certificate)\n\n        store.set_time(sign_date)\n        store_ctx = X509StoreContext(store, cert_ossl)\n        try:\n            store_ctx.verify_certificate()\n        except X509StoreContextError as store_ctx_error:\n            return CertificateVerificationFailure(\n                exception=store_ctx_error,\n            )\n\n        # 2) Check that the signing certificate contains the proof claim as the subject\n        # Check usage is \"digital signature\"\n        usage_ext = materials.certificate.extensions.get_extension_for_class(KeyUsage)\n        if not usage_ext.value.digital_signature:\n            return VerificationFailure(\n                reason=\"Key usage is not of type `digital signature`\"\n            )\n\n        # Check that extended usage contains \"code signing\"\n        extended_usage_ext = materials.certificate.extensions.get_extension_for_class(\n            ExtendedKeyUsage\n        )\n        if ExtendedKeyUsageOID.CODE_SIGNING not in extended_usage_ext.value:\n            return VerificationFailure(\n                reason=\"Extended usage does not contain `code signing`\"\n            )\n\n        policy_check = policy.verify(materials.certificate)\n        if not policy_check:\n            return policy_check\n\n        logger.debug(\"Successfully verified signing certificate validity...\")\n\n        # 3) Verify that the signature was signed by the public key in the signing certificate\n        try:\n            signing_key = materials.certificate.public_key()\n            signing_key = cast(ec.EllipticCurvePublicKey, signing_key)\n            signing_key.verify(\n                materials.signature,\n                materials.input_digest,\n                ec.ECDSA(Prehashed(hashes.SHA256())),\n            )\n        except InvalidSignature:\n            return VerificationFailure(reason=\"Signature is invalid for input\")\n\n        logger.debug(\"Successfully verified signature...\")\n\n        # 4) Retrieve the Rekor entry for this artifact (potentially from\n        # an offline entry), confirming its consistency with the other\n        # artifacts in the process.\n        try:\n            entry = materials.rekor_entry(self._rekor)\n        except RekorEntryMissingError:\n            return LogEntryMissing(\n                signature=B64Str(base64.b64encode(materials.signature).decode()),\n                artifact_hash=HexStr(materials.input_digest.hex()),\n            )\n        except InvalidRekorEntryError:\n            return VerificationFailure(\n                reason=\"Rekor entry contents do not match other signing materials\"\n            )\n\n        # 5) Verify the inclusion proof supplied by Rekor for this artifact.\n        #\n        # The inclusion proof should always be present in the online case. In\n        # the offline case, if it is present, we verify it.\n        if entry.inclusion_proof and entry.inclusion_proof.checkpoint:\n            try:\n                verify_merkle_inclusion(entry)\n            except InvalidInclusionProofError as exc:\n                return VerificationFailure(\n                    reason=f\"invalid Rekor inclusion proof: {exc}\"\n                )\n\n            try:\n                verify_checkpoint(self._rekor, entry)\n            except CheckpointError as exc:\n                return VerificationFailure(reason=f\"invalid Rekor root hash: {exc}\")\n\n            logger.debug(\n                f\"successfully verified inclusion proof: index={entry.log_index}\"\n            )\n        elif not materials._offline:\n            # Paranoia: if we weren't given an inclusion proof, then\n            # this *must* have been offline verification. If it was online\n            # then we've somehow entered an invalid state, so fail.\n            return VerificationFailure(reason=\"missing Rekor inclusion proof\")\n        else:\n            logger.warning(\n                \"inclusion proof not present in bundle: skipping due to offline verification\"\n            )\n\n        # 6) Verify the Signed Entry Timestamp (SET) supplied by Rekor for this artifact\n        if entry.inclusion_promise:\n            try:\n                verify_set(self._rekor, entry)\n                logger.debug(\n                    f\"successfully verified inclusion promise: index={entry.log_index}\"\n                )\n            except InvalidSETError as inval_set:\n                return VerificationFailure(\n                    reason=f\"invalid Rekor entry SET: {inval_set}\"\n                )\n\n        # 7) Verify that the signing certificate was valid at the time of signing\n        integrated_time = datetime.datetime.utcfromtimestamp(entry.integrated_time)\n        if not (\n            materials.certificate.not_valid_before\n            <= integrated_time\n            <= materials.certificate.not_valid_after\n        ):\n            return VerificationFailure(\n                reason=\"invalid signing cert: expired at time of Rekor entry\"\n            )\n\n        return VerificationSuccess()"
            ],
            "documentation": [
                "\nVerification API machinery.\n"
            ],
            "other": [
                "# Copyright 2022 The Sigstore Authors",
                "#",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");",
                "# you may not use this file except in compliance with the License.",
                "# You may obtain a copy of the License at",
                "#",
                "#      http://www.apache.org/licenses/LICENSE-2.0",
                "#",
                "# Unless required by applicable law or agreed to in writing, software",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "# See the License for the specific language governing permissions and",
                "# limitations under the License.",
                "from __future__ import annotations",
                "from typing import List, cast",
                "from cryptography.exceptions import InvalidSignature",
                "from cryptography.hazmat.primitives import hashes",
                "from cryptography.hazmat.primitives.asymmetric import ec",
                "from cryptography.hazmat.primitives.asymmetric.utils import Prehashed",
                "from cryptography.x509 import Certificate, ExtendedKeyUsage, KeyUsage",
                "from cryptography.x509.oid import ExtendedKeyUsageOID",
                "from OpenSSL.crypto import (  # type: ignore[import-untyped]\n    X509,\n    X509Store,\n    X509StoreContext,\n    X509StoreContextError,\n)",
                "from pydantic import ConfigDict",
                "from sigstore._internal.merkle import (\n    InvalidInclusionProofError,\n    verify_merkle_inclusion,\n)",
                "from sigstore._internal.rekor.checkpoint import (\n    CheckpointError,\n    verify_checkpoint,\n)",
                "from sigstore._internal.rekor.client import RekorClient",
                "from sigstore._internal.set import InvalidSETError, verify_set",
                "from sigstore._internal.tuf import TrustUpdater",
                "from sigstore._utils import B64Str, HexStr",
                "from sigstore.verify.models import InvalidRekorEntry as InvalidRekorEntryError",
                "from sigstore.verify.models import RekorEntryMissing as RekorEntryMissingError",
                "from sigstore.verify.models import (\n    VerificationFailure,\n    VerificationMaterials,\n    VerificationResult,\n    VerificationSuccess,\n)",
                "from sigstore.verify.policy import VerificationPolicy",
                "logger = logging.getLogger(__name__)"
            ],
            "functions_code": [],
            "functions_docstrings": [],
            "classes_code": [
                "class LogEntryMissing(VerificationFailure):\n    \n\n    reason: (\n        str\n    ) = \"The transparency log has no entry for the given verification materials\"\n\n    signature: B64Str\n    \n\n    artifact_hash: HexStr\n    ",
                "class CertificateVerificationFailure(VerificationFailure):\n    \n\n    # Needed for the `exception` field above, since exceptions are\n    # not trivially serializable.\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    reason: str = \"Failed to verify signing certificate\"\n    exception: Exception",
                "class Verifier:\n    \n\n    def __init__(\n        self, *, rekor: RekorClient, fulcio_certificate_chain: List[Certificate]\n    ):\n        \n        self._rekor = rekor\n\n        self._fulcio_certificate_chain: List[X509] = []\n        for parent_cert in fulcio_certificate_chain:\n            parent_cert_ossl = X509.from_cryptography(parent_cert)\n            self._fulcio_certificate_chain.append(parent_cert_ossl)\n\n    @classmethod\n    def production(cls) -> Verifier:\n        \n        updater = TrustUpdater.production()\n        return cls(\n            rekor=RekorClient.production(updater),\n            fulcio_certificate_chain=updater.get_fulcio_certs(),\n        )\n\n    @classmethod\n    def staging(cls) -> Verifier:\n        \n        updater = TrustUpdater.staging()\n        return cls(\n            rekor=RekorClient.staging(updater),\n            fulcio_certificate_chain=updater.get_fulcio_certs(),\n        )\n\n    def verify(\n        self,\n        materials: VerificationMaterials,\n        policy: VerificationPolicy,\n    ) -> VerificationResult:\n        \n\n        # NOTE: The `X509Store` object currently cannot have its time reset once the `set_time`\n        # method been called on it. To get around this, we construct a new one for every `verify`\n        # call.\n        store = X509Store()\n        for parent_cert_ossl in self._fulcio_certificate_chain:\n            store.add_cert(parent_cert_ossl)\n\n        # In order to verify an artifact, we need to achieve the following:\n        #\n        # 1) Verify that the signing certificate is signed by the certificate\n        #    chain and that the signing certificate was valid at the time\n        #    of signing.\n        # 2) Verify that the signing certificate belongs to the signer.\n        # 3) Verify that the artifact signature was signed by the public key in the\n        #    signing certificate.\n        # 4) Verify that the Rekor entry is consistent with the other signing\n        #    materials (preventing CVE-2022-36056)\n        # 5) Verify the inclusion proof supplied by Rekor for this artifact,\n        #    if we're doing online verification.\n        # 6) Verify the Signed Entry Timestamp (SET) supplied by Rekor for this\n        #    artifact.\n        # 7) Verify that the signing certificate was valid at the time of\n        #    signing by comparing the expiry against the integrated timestamp.\n\n        # 1) Verify that the signing certificate is signed by the root certificate and that the\n        #    signing certificate was valid at the time of signing.\n        sign_date = materials.certificate.not_valid_before\n        cert_ossl = X509.from_cryptography(materials.certificate)\n\n        store.set_time(sign_date)\n        store_ctx = X509StoreContext(store, cert_ossl)\n        try:\n            store_ctx.verify_certificate()\n        except X509StoreContextError as store_ctx_error:\n            return CertificateVerificationFailure(\n                exception=store_ctx_error,\n            )\n\n        # 2) Check that the signing certificate contains the proof claim as the subject\n        # Check usage is \"digital signature\"\n        usage_ext = materials.certificate.extensions.get_extension_for_class(KeyUsage)\n        if not usage_ext.value.digital_signature:\n            return VerificationFailure(\n                reason=\"Key usage is not of type `digital signature`\"\n            )\n\n        # Check that extended usage contains \"code signing\"\n        extended_usage_ext = materials.certificate.extensions.get_extension_for_class(\n            ExtendedKeyUsage\n        )\n        if ExtendedKeyUsageOID.CODE_SIGNING not in extended_usage_ext.value:\n            return VerificationFailure(\n                reason=\"Extended usage does not contain `code signing`\"\n            )\n\n        policy_check = policy.verify(materials.certificate)\n        if not policy_check:\n            return policy_check\n\n        logger.debug(\"Successfully verified signing certificate validity...\")\n\n        # 3) Verify that the signature was signed by the public key in the signing certificate\n        try:\n            signing_key = materials.certificate.public_key()\n            signing_key = cast(ec.EllipticCurvePublicKey, signing_key)\n            signing_key.verify(\n                materials.signature,\n                materials.input_digest,\n                ec.ECDSA(Prehashed(hashes.SHA256())),\n            )\n        except InvalidSignature:\n            return VerificationFailure(reason=\"Signature is invalid for input\")\n\n        logger.debug(\"Successfully verified signature...\")\n\n        # 4) Retrieve the Rekor entry for this artifact (potentially from\n        # an offline entry), confirming its consistency with the other\n        # artifacts in the process.\n        try:\n            entry = materials.rekor_entry(self._rekor)\n        except RekorEntryMissingError:\n            return LogEntryMissing(\n                signature=B64Str(base64.b64encode(materials.signature).decode()),\n                artifact_hash=HexStr(materials.input_digest.hex()),\n            )\n        except InvalidRekorEntryError:\n            return VerificationFailure(\n                reason=\"Rekor entry contents do not match other signing materials\"\n            )\n\n        # 5) Verify the inclusion proof supplied by Rekor for this artifact.\n        #\n        # The inclusion proof should always be present in the online case. In\n        # the offline case, if it is present, we verify it.\n        if entry.inclusion_proof and entry.inclusion_proof.checkpoint:\n            try:\n                verify_merkle_inclusion(entry)\n            except InvalidInclusionProofError as exc:\n                return VerificationFailure(\n                    reason=f\"invalid Rekor inclusion proof: {exc}\"\n                )\n\n            try:\n                verify_checkpoint(self._rekor, entry)\n            except CheckpointError as exc:\n                return VerificationFailure(reason=f\"invalid Rekor root hash: {exc}\")\n\n            logger.debug(\n                f\"successfully verified inclusion proof: index={entry.log_index}\"\n            )\n        elif not materials._offline:\n            # Paranoia: if we weren't given an inclusion proof, then\n            # this *must* have been offline verification. If it was online\n            # then we've somehow entered an invalid state, so fail.\n            return VerificationFailure(reason=\"missing Rekor inclusion proof\")\n        else:\n            logger.warning(\n                \"inclusion proof not present in bundle: skipping due to offline verification\"\n            )\n\n        # 6) Verify the Signed Entry Timestamp (SET) supplied by Rekor for this artifact\n        if entry.inclusion_promise:\n            try:\n                verify_set(self._rekor, entry)\n                logger.debug(\n                    f\"successfully verified inclusion promise: index={entry.log_index}\"\n                )\n            except InvalidSETError as inval_set:\n                return VerificationFailure(\n                    reason=f\"invalid Rekor entry SET: {inval_set}\"\n                )\n\n        # 7) Verify that the signing certificate was valid at the time of signing\n        integrated_time = datetime.datetime.utcfromtimestamp(entry.integrated_time)\n        if not (\n            materials.certificate.not_valid_before\n            <= integrated_time\n            <= materials.certificate.not_valid_after\n        ):\n            return VerificationFailure(\n                reason=\"invalid signing cert: expired at time of Rekor entry\"\n            )\n\n        return VerificationSuccess()"
            ],
            "classes_docstrings": [
                "\n    A specialization of `VerificationFailure` for transparency log lookup failures,\n    with additional lookup context.\n    ",
                "\n    The signature present during lookup failure, encoded with base64.\n    ",
                "\n    The artifact hash present during lookup failure, encoded as a hex string.\n    ",
                "\n    A specialization of `VerificationFailure` for certificate signature\n    verification failures, with additional exception context.\n    ",
                "\n    The primary API for verification operations.\n    ",
                "\n        Create a new `Verifier`.\n\n        `rekor` is a `RekorClient` capable of connecting to a Rekor instance\n        containing logs for the file(s) being verified.\n\n        `fulcio_certificate_chain` is a list of PEM-encoded X.509 certificates,\n        establishing the trust chain for the signing certificate and signature.\n        ",
                "\n        Return a `Verifier` instance configured against Sigstore's production-level services.\n        ",
                "\n        Return a `Verifier` instance configured against Sigstore's staging-level services.\n        ",
                "Public API for verifying.\n\n        `materials` are the `VerificationMaterials` to verify.\n\n        `policy` is the `VerificationPolicy` to verify against.\n\n        Returns a `VerificationResult` which will be truthy or falsey depending on\n        success.\n        "
            ]
        }
    }
}